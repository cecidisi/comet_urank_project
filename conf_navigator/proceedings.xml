<?xml version="1.0"?>
<RECORDS><RECORD><contentID>12560</contentID><title>A VR Simulator for Emergency Management in Endodontic Surgery</title><contentType>Demo</contentType><presentationID>10529</presentationID><abstract>We present a virtual reality simulator for teaching emergency management decision-making in endodontic surgery. Objectives of the simulator are to 1) teach how to correctly respond to a variety of emergency situations, 2) acclimate students to making decisions in stressful emergency situations and 3) teach students the situation awareness skills required to rapidly recognize and respond to emergencies.  To meet these objectives, we present a simulator that permits emergency situations to be dynamically inserted at various points in the procedure and that is immersive. The simulator also allows a teacher to observe and review a session in real-time or post session. Preliminary evaluation of face and content validity shows that the simulation is sufficiently realistic and the system is a promising teaching tool.</abstract><author_list>Nat Sararit, Peter Haddawy, Siriwan Suebnukarn</author_list></RECORD><RECORD><contentID>12559</contentID><title>AMuse: Connecting Indoor and Outdoor Cultural Heritage Experiences</title><contentType>Demo</contentType><presentationID>10531</presentationID><abstract>The following discusses a demo for an application that serves as a museum guide, which after the visit gives advice on additional cultural heritage sites to visit. The demo simulates the implementation at the Tower of David in Jerusalem. The system uses behavior to determine both preferences and characteristics.</abstract><author_list>Alan Wecker, Alan Wecker, Tsvi Kuflik, Oliviero Stock</author_list></RECORD><RECORD><contentID>12564</contentID><title>Concept-Level Knowledge Visualization For Supporting Self-Regulated Learning</title><contentType>Demo</contentType><presentationID>10537</presentationID><abstract>Mastery Grids is an intelligent interface that provides access to different kinds of practice content for an introductory programming course. A distinctive feature of the interface is a parallel topic-level visualization of student progress and the progress of their peers. This contribution presents an extended version of the original system that features a fine-grained visualization of student knowledge on the level of the detailed concepts that are associated with the course. The student model is based on a Bayesian-network which is built using students performance history in the learning activities.</abstract><author_list>Jordan Barria-Pineda, Julio Guerra, Julio Guerra, Yun Huang, Peter Brusilovsky</author_list></RECORD><RECORD><contentID>12566</contentID><title>Design of Playful Authoring Tools for Social and Behavioral Science</title><contentType>Demo</contentType><presentationID>10528</presentationID><abstract>Playful environments are increasingly being used for conducting research. This makes a game platform for authoring research studies and teaching about how to conduct research a necessary progression. In this paper, we discuss Mad Science, a playful platform that is being created to allow users to create behavioral experiments. We discuss iterations of the authoring tools, including lessons learned, and the need for AI assistance to guide and teach users.</abstract><author_list>Casper Harteveld, Nolan Manning, Farah Abu-Arja, Rick Menasce, Dean Thurston, Gillian Smith, Steven Sutherland</author_list></RECORD><RECORD><contentID>12567</contentID><title>EHCTool: Managing Emotional Hotspots for Conversational Agents</title><contentType>Demo</contentType><presentationID>10533</presentationID><abstract>Building conversational agents is becoming easier thanks to the profusion of designated platforms. Integrating emotional intelligence in such agents contributes to positive user satisfaction. Currently, this integration is implemented using calls to an emotion analysis service. In this demonstration we present EHCTool that aims to detect and notify the conversation designer about problematic conversation states where emotions are likely to be expressed by the user. Using its exploration view, the tool assists the designer to manage and define appropriate responses in these cases.</abstract><author_list>Tommy Sandbank, Michal Shmueli-Scheuer, Jonathan Herzig, David Konopnicki, Rottem Shaul</author_list></RECORD><RECORD><contentID>12571</contentID><title>GazeTheKey: Interactive Keys to Integrate Word Predictions for Gaze-based Text Entry</title><contentType>Demo</contentType><presentationID>10530</presentationID><abstract>In the conventional keyboard interfaces for eye typing, the functionalities of the virtual keys are static, i.e., user&#x2019;s gaze at a particular key simply translates the associated letter as user&#x2019;s input. In this work we argue the keys to be more dynamic and embed intelligent predictions to support gaze-based text entry. In this regard, we demonstrate a novel "GazeTheKey" interface where a key not only signifies the input character, but also predict the relevant words that could be selected by user's gaze utilizing a two-step dwell time. </abstract><author_list>Korok Sengupta, Raphael Menges, Chandan Kumar, Steffen Staab</author_list></RECORD><RECORD><contentID>12572</contentID><title>Improving Search Result Comprehension by Topic-Relevance Map Visualization</title><contentType>Demo</contentType><presentationID>10534</presentationID><abstract>We introduce topic-relevance map, an interactive search result visualization that assists rapid information comprehension across a large ranked set of results. The topic-relevance map visualizes a topical overview of the search result space as keywords with respect to two essential information retrieval measures: relevance and topical similarity. Non-linear dimensionality reduction is used to embed high-dimensional keyword representations of search result data into angles on a radial layout. Relevance of keywords is estimated by a ranking method and visualized as radiuses on the layout. Similar keywords are modeled by nearby points and more relevant keywords are closer to the center of the radial display. We evaluated the effect of the topic-relevance map in a search result comprehension task where 24 participants were summarizing search results and produced a conceptualization of the result space. Topic-relevance map significantly improves participants\' comprehension capability compared to a ranked list.</abstract><author_list>Jaakko Peltonen, Kseniia Belorustceva, Tuukka Ruotsalo</author_list></RECORD><RECORD><contentID>12574</contentID><title>Modular Audio Story Platform for Museums</title><contentType>Demo</contentType><presentationID>10525</presentationID><abstract>Museums are seeking different ways to attract and engage audiences. Digital stories in various forms have been utilized as one approach to increase audience experience. This paper presents how to bring audio stories as a part of museum&#xC3;&#xA2;&#xE2;?&#xAC;&#xE2;?&#xA2;s activities by developing a modular audio story platform. Most of the functionality is included in Android applications, which allow visitors to attach stories with emotions to artifacts, share stories with other visitors and enrich existing stories with sounds. All the audio files, linking of the artifacts and related audio files are managed by audio digital asset management system. Our platform supports curated audio stories, but the main emphasis is in the visitors&#xC3;&#xA2;&#xE2;?&#xAC;&#xE2;?&#xA2; audio stories. We differentiate from the other digital storytelling systems by attaching emotions onto the visitor stories, and combining the soundscapes and audio stories as visitor modified audio stories.</abstract><author_list>Kari Salo, Vallo Zinin, Merja Bauters, Tommi Mikkonen</author_list></RECORD><RECORD><contentID>12576</contentID><title>Motion-Based Serious Games for Hand Assistive Rehabilitation</title><contentType>Demo</contentType><presentationID>10527</presentationID><abstract>Cerebral Palsy, trauma, and strokes are common causes for the loss of hand movements and the decrease in muscle strength for both children and adults. Improving fine motor skills usually involves the synchronization of wrists and fingers by performing appropriate tasks and activities. This demo introduces a novel patient-centered framework for the gamification of hand therapies in order to facilitate and encourage the rehabilitation process. This framework consists of an adaptive therapy-driven 3D environment augmented with our motion-based natural user interface. An intelligent game generator is developed, which translates the patient\'s gestures into navigational movements with therapy-driven goals, while adapting the level of difficulty based on the patient profile and real-time performance. A comprehensive evaluation and clinical-based assessments were conducted in a local children disability center, and highlights of the results are presented.</abstract><author_list>Imad Afyouni, Ahmad Qamar, Syed Osama Hussain, Faizan Ur Rehman, Bilal Sadiq, Abdullah Murad</author_list></RECORD><RECORD><contentID>12578</contentID><title>SEED: Entity Oriented Information Search and Exploration</title><contentType>Demo</contentType><presentationID>10526</presentationID><abstract>Entity search and exploration can enrich search user interfaces by presenting relevant information instantly and offering relevant exploration pointers to users. Previous research has demonstrated that large Knowledge Graphs allow exploitation and recommendation of explicit links between the entities and other information to improve information access and ranking. However, less attention has been devoted to user interfaces for effectively presenting results, recommending related entities and explaining relations between entities. We introduce a system called SEED which is designed to support entity search and exploration in large Knowledge Graphs. We demonstrate SEED using a dataset of hundreds of thousands of movie related entities from the DBpedia Knowledge Graph. The system utilizes a graph embedding model for ranking entities and their relations, recommending related entities, and explaining their interrelations.</abstract><author_list>Jun Chen, Giulio Jacucci, Yueguo Chen, Tuukka Ruotsalo</author_list></RECORD><RECORD><contentID>12582</contentID><title>Supporting Conference Attendees with Visual Decision Making Interfaces</title><contentType>Demo</contentType><presentationID>10535</presentationID><abstract>Recent efforts in recommender systems research focus increasingly on human factors affecting recommendation acceptance, such as transparency and user control. In this paper, we present IntersectionExplorer, a scalable visualization to interleave the output of several recommender engines with user-contributed relevance information, such as bookmarks and tags. Two user studies at conferences indicate that this approach is well suited for technical audiences in smaller venues, and allowed the identification of applicability limitations for less technical audiences attending larger events.</abstract><author_list>Bruno Cardoso, Peter Brusilovsky, Chirayu Wongchokprasitti, Denis Parra, Katrien Verbert</author_list></RECORD><RECORD><contentID>12584</contentID><title>TweetVista: An AI-Powered Interactive Tool for Exploring Conversations on Twitter</title><contentType>Demo</contentType><presentationID>10532</presentationID><abstract>We present TweetVista, an interactive web-based tool for mapping the conversation landscapes on Twitter. TweetVista is an intelligent and interactive desktop web application for exploring the conversation landscapes on Twitter. Given a dataset of tweets, the tool uses advanced NLP techniques using deep neural networks and a scalable clustering algorithm to map out coherent conversation clusters. The interactive visualization engine then enables the users to explore these clusters. We ran three case studies using datasets about the 2016 US presidential election and the summer 2016 Orlando shooting. Despite the enormous size of these datasets, using TweetVista users were able to quickly and clearly make sense of the various conversation topics around these datasets.</abstract><author_list>Prashanth Vijayaraghavan, Soroush Vosoughi, Ann Yuan, Deb Roy</author_list></RECORD><RECORD><contentID>12586</contentID><title>Visual Exploration of Unstructured Regulatory Documents</title><contentType>Demo</contentType><presentationID>10536</presentationID><abstract>Governmental authorities publish rules and directives that govern the operations of an industry. These documents, called regulations, are meant to safeguard the interests of consumers. With increasing number, size and complexity of such documents, companies face an uphill task to comply with them. We present a cognitive system, called Cogpliance, for exploring and understanding regulatory documents with the goal of assisting compliance officers in attaining regulatory compliance. Cogpliance automatically reads natural language regulatory documents, extracts key concepts and presents an interactive information exploration user interface for answering compliance officers queries.</abstract><author_list>Nishtha Madaan, Hima Karanam, Ankush Gupta, Nitisha Jain, Arun Kumar, Srikanth Tamilselvam</author_list></RECORD><RECORD><contentID>12657</contentID><title>A Framework of Health Information Retrieval for Aging Population</title><contentType>Doctoral Consortium</contentType><presentationID>10639</presentationID><abstract>Aging populations have a huge demand of searching health information online. However, for their relatively worse physical ability and cognitive ability, normal searching interface may not be able to fulfill aging people\'s special demands. In our work, we point out the problem which aging populations are facing when they use the normal online searching system. Then, we propose our interactive health information retrieval framework for aging populations using actual pages from the WebMD.com, a popular website where people search for health information. There are three phases in our proposed framework: the retrieval model design, the interface design and the evaluation design. We hope our interface could help aging users obtain better experience when they search for healthcare information online.</abstract><author_list>Mingkun Gao</author_list></RECORD><RECORD><contentID>12646</contentID><title>Adapting at Run-time: Exploring the Design Space of Personalized Fitness Coaches</title><contentType>Doctoral Consortium</contentType><presentationID>10647</presentationID><abstract>Personal health and fitness technologies, such as activity trackers, bear the potential to impact health behaviors globally.   However, most users abandon these technology quickly. Possible reasons are that provided feedback (often consisting of raw data) is not actionable, not relevant, or the provided advice is not easy to integrate into people\'s lives. One approach to tackle this problem, is to develop personalized or adaptive digital coaches that take users\' individual differences and situation into account. Even though the first prototypes of personalized coaches have been presented and evaluated, this research is still in its infancy. In my thesis, I want to extend this research by (a) investigating the influences of individual differences on behaviors and motivations to use a digital fitness coach, (2) mapping and conceptually exploring the design space of personalized digital fitness coaches, (3) and iteratively prototyping and testing adaptations of personalized fitness coaches in a user-centered design process.</abstract><author_list>Hanna Schneider</author_list></RECORD><RECORD><contentID>12659</contentID><title>Advanced User Interfaces and Hybrid Recommendations for Exploratory Search</title><contentType>Doctoral Consortium</contentType><presentationID>10640</presentationID><abstract>Exploring large volumes of data with learning or investigative purposes is often regarded as exploratory search. Rather than plain question answering, exploratory search is an iterative process of information seeking and sensemaking. My focus is the development of an interactive intelligent tool that assists the search task and the study of user behavior and experience. More specifically, I combine recommender systems with advanced user interfaces to maximize what Hearst calls \</abstract><author_list>Cecilia di Sciascio</author_list></RECORD><RECORD><contentID>12660</contentID><title>An Interactive and Interpretable Interface for Diversity in Recommender Systems</title><contentType>Doctoral Consortium</contentType><presentationID>10653</presentationID><abstract>Offering diversity in the output of a recommender system is an active research question. Most of the current approaches focus on Top-N optimization, which results in poor user insight and accuracy trade-off. However, little is known about how an interactive interface can help with this issue. This pilot study shows that a multidimensional visualization promotes diversity among the recommended items. This finding motivated future work to provide diversity in recommender system by visualizing multivariate data through an interpretable and interactive interface.</abstract><author_list>Chun-Hua Tsai</author_list></RECORD><RECORD><contentID>12651</contentID><title>Automated Formative Feedback in a Virtual Reality (VR) Dental Surgery Simulator</title><contentType>Doctoral Consortium</contentType><presentationID>10642</presentationID><abstract>Fine motor skill is indispensable for a dentist. As in many other medical fields of study, the traditional surgical master apprentice model is widely adopted in dental education. Recently, virtual reality (VR) simulators have been employed as supplementary components to the traditional skill-training curriculum, and numerous dental VR systems have been developed academically and commercially. However, the full promise of such systems has yet to be realized due to the lack of sufficient support for formative feedback. Without such a mechanism, evaluation still demands dedicated time of experts in scarce supply. With the aim to fill the gap of formative assessment using VR simulators in skill training in dentistry, we propose a framework to objectively assess the surgical skill and generate feedback automatically. The core concept of the framework is to generate the feedback by correlating the portion of the procedure responsible with the error in the outcome. Assessment of outcome and the procedure, pedagogical models, and multiple modalities to provide feedback are the integral components of this research.</abstract><author_list>Myat Su Yin</author_list></RECORD><RECORD><contentID>12650</contentID><title>Characterizing and Modeling Linguistic Style in Dialogue for Intelligent Social Agents</title><contentType>Doctoral Consortium</contentType><presentationID>10649</presentationID><abstract>With increasing interest in the development of intelligent agents capable of learning, proficiently automating tasks, and gaining world knowledge, the importance of integrating the ability to converse naturally with users is more crucial now than ever before. This thesis aims to understand and characterize different aspects of social language to facilitate the development of intelligent agents that are socially aware and able to engage users to a level that was not previously possible with language generation systems. Using various machine learning algorithms and data-driven approaches to model the nuances of social language in dialogue, such as factual and emotional expression, sarcasm and humor and the related subclasses of rhetorical questions and hyperbole, we can come closer to modeling the characteristics of the social language that allows us to express emotion and knowledge, and thereby exhibit these styles in the agents we develop.</abstract><author_list>Shereen Oraby</author_list></RECORD><RECORD><contentID>12643</contentID><title>Developing a User-defined Interface for In-vehicle Mid-air Gestural Interactions</title><contentType>Doctoral Consortium</contentType><presentationID>10646</presentationID><abstract>Despite the recent developments in gesture-driven technologies facilitating multi-touch and mid-air gesture recognition, there has been little formal user evaluation and analysis of these systems for in-vehicle interfaces. Mid-air gesture-based interfaces can provide a less cumbersome in-vehicle interface for safer driving. Recent developments in gesture-driven technologies have facilitated multi-touch and mid-air gesture recognition. However, for in-vehicle interfaces, research needs to be conducted on the most efficient gesture vocabulary for performing secondary tasks. Following the Interaction Design process user requirements need to be explored, followed by evaluation of characteristics and functions. Then, the outcomes of user evaluation study can be used to develop an efficient in-vehicle gestural interface.</abstract><author_list>Hessam Jahani-Fariman</author_list></RECORD><RECORD><contentID>12654</contentID><title>Emergency Management Integration in an Endodontic Surgery VR Simulator</title><contentType>Doctoral Consortium</contentType><presentationID>10644</presentationID><abstract>We present the design and prototype implementation of a virtual reality simulator for teaching emergency management decision-making in endodontic surgery. The simulator aims to teach how to correctly respond to a variety of emergency situations and teach students the situation awareness skills required to rapidly recognize and respond to emergencies.  We present results of an initial evaluation of face and content validity of the prototype and describe the design of a more comprehensive evaluation including evaluating the knowledge gain in emergency management of students trained with the system as well as the effectiveness of simulator in inducing a feeling of stress in students.</abstract><author_list>Nat Sararit</author_list></RECORD><RECORD><contentID>12647</contentID><title>EpistAid: An Interactive Intelligent System for Evidence-based Health Care</title><contentType>Doctoral Consortium</contentType><presentationID>10638</presentationID><abstract>Evidence-based health care (EBHC) is an important practice of medicine which provides systematic scientific evidence to answer clinical questions. Epistemonikos is one of the most important online systems in the field. Currently, many tasks within this system require a large amount of manual effort, which could be improved by leveraging human-in-the-loop machine learning techniques. In this article we propose a system called EpistAid, which combines machine learning, relevance feedback and an interactive user interface to support Epistemonikos users\' on EBHC information filtering tasks.</abstract><author_list>Ivania Donoso-Guzm&#xE1;n</author_list></RECORD><RECORD><contentID>12652</contentID><title>Game Features and Individual Differences: What Makes a Spatial Skill Training Video Game Effective?</title><contentType>Doctoral Consortium</contentType><presentationID>10643</presentationID><abstract>This document gives an overview of my current research project investigating how children develop spatial reasoning skills through video game training. I describe the motivation and goals of the project and the progress made so far.</abstract><author_list>Helen Wauck</author_list></RECORD><RECORD><contentID>12649</contentID><title>Gesture Recognition through Declarative and Classifier Approach</title><contentType>Doctoral Consortium</contentType><presentationID>10648</presentationID><abstract>Now, users can easily provide input relying on body movements through the newest tracking devices. The available solutions have a mismatch: on one hand, classifiers offer a high precision, but their structure is difficult to inspect for providing feedback and feedforward. On the other hand, compositional approaches for gesture definition support decomposition, but with a low recognition precision. We introduce DEICTIC, a compositional and declarative gesture description that allows creating Hidden Markov Models (HMMs) for recognizing a gesture precisely, while providing information on its sub-components.</abstract><author_list>Alessandro Carcangiu</author_list></RECORD><RECORD><contentID>12653</contentID><title>Implicit Acquisition of User Personality for Augmenting Recommender Systems</title><contentType>Doctoral Consortium</contentType><presentationID>10652</presentationID><abstract>In recent years, user personality has been increasingly recognized as a valuable resource being incorporated into the process of generating recommendations. However, the effort of explicitly acquiring users\' personality traits via psychological questionnaire is unavoidably high, which may impede the application of personality-based recommenders in real life. My PhD research aims to investigate how to derive users\' personality from their implicit behavior and further improve the existing recommender systems. For this purpose, we first identify significant features through experimental validation. We then build inference model to unify these features for determining users\' Big-Five personality traits. We further develop personalized recommender systems by incorporating the inferred personality. Our study would indicate an effective solution to boost the applicability of personality-based recommender systems in the online environment.</abstract><author_list>Wen Wu</author_list></RECORD><RECORD><contentID>12648</contentID><title>Intelligent and Personalized Community Maps</title><contentType>Doctoral Consortium</contentType><presentationID>10651</presentationID><abstract>My PhD project focuses on Participatory GIS (PGIS). In the project I analyze two methodologies to offer personalized search results in community maps and a natural interaction with the system. The first consists of automatically gathering the terms according to which the users express their information needs, in order to enrich the domain conceptualization of a PGIS, giving common definitions for places. The second concerns the creation of ontology-based user models that reflect the interests, lexicon and modality of expression adopted by each person, mapped to the domain ontology adopted by the PGIS. In the project I also analyze how these techniques may be jointly used during the query expansion process to retrieve more accurate and relevant search results.</abstract><author_list>Noemi Mauro</author_list></RECORD><RECORD><contentID>12658</contentID><title>Intelligent Interface for Seeing the World Through Different Lenses</title><contentType>Doctoral Consortium</contentType><presentationID>10641</presentationID><abstract>Despite the explosive growth of online social media where people can easily share their thoughts, our current society is more divided than before, gridlocked over society, culture, race, and gender issues. Selective exposure, a confirmatory bias of individuals that favors preexisting opinions while avoiding attitude-inconsistent views, impedes the balanced insight of a controversial issue, which thereby would account for the societal division. In this research proposal, we introduce an intelligent interface that automatically clusters and visualizes diverse opinions about a controversial topic. First, we collect controversial posts from Facebook and its comments. Then, the comments are automatically clustered using a machine-learning algorithm based on features that reflect its contents and the writer\'s stance. Lastly, we propose an intelligent user interface with controversial posts and opinion clusters where users would be motivated to hunt for opinion groups that are different from their own perspective.</abstract><author_list>Hyo Jin Do</author_list></RECORD><RECORD><contentID>12644</contentID><title>Pairwise Preferences and Recommender Systems</title><contentType>Doctoral Consortium</contentType><presentationID>10650</presentationID><abstract>Most of the present research and application of Recommender Systems is based on the usage of preferences derived from absolute evaluations, such as user ratings or clicks. However, this type of preferences has few disadvantages, e.g., if most of the user rated items are 5 stars, then it is difficult to understand which item the user prefers among them. In this research work, we focus on pairwise preferences as an alternative way for modeling user preferences and compute recommendations. In our scenario, users provide pair scores for a set of item pairs, indicating which item, and to what extent, is preferred. In this Ph.D research, we aim at developing intelligent user interfaces that optimally combine ratings with pairwise preferences. Furthermore, we aim at identifying specific conditions/situations where pairwise preferences elicitation is meaningful and beneficial.</abstract><author_list>Saikishore Kalloori</author_list></RECORD><RECORD><contentID>12655</contentID><title>Personalization of Virtual Games for Children with Cerebral Palsy</title><contentType>Doctoral Consortium</contentType><presentationID>10645</presentationID><abstract>The purpose of the current work is to explore the potential of a personalized virtual gaming system capable of dynamically adjusting game parameters in accordance with the abilities and therapeutic needs of children with Cerebral Palsy (CP). The study includes three stages: Defining user characteristics and identifying user requirements of children with CP in order to create a user model via interviews and focus groups; Developing a prototype of a personalized virtual gaming system for rehabilitation of children with CP and; Evaluating the prototype with typically developing children and children with CP. Initial results from the first stage identified the benefits and potential of personalized virtual gaming for treating children with CP from both the therapist&#xE2;??s and child&#xE2;??s viewpoints. Iterative prototyping and testing of the personalization algorithm are currently in progress.</abstract><author_list>Sarit Tresser</author_list></RECORD><RECORD><contentID>12597</contentID><title>Interaction Design for Rehabiliation</title><contentType>Keynote</contentType><presentationID>10538</presentationID><abstract>Well-known trends pertaining to the aging of population and the rising costs of healthcare motivate the development of rehabilitation technology. There is a considerable body of work in this area including efforts to make serious games, virtual reality and robotic applications. While innovative technologies have been introduced over the years, and often researchers produce promising experimental results, these technologies have not yet delivered the anticipated benefits.

The causes for this apparent failure are evident when looking a closer look at the case of stroke rehabilitation, which is one of the heaviest researched topics for developing rehabilitation technologies. It is argued that improvements should be sought by centering the design on an understanding of patient needs, allowing patients, therapists and care givers in general to personalize solutions to the need of patients, effective feedback and motivation strategies to be implemented, and an in depth understanding of the socio-technical system in which the rehabilitation technology will be embedded. These are classic challenges that human computer interaction (HCI) researchers have been dealing with for years, which is why the field of rehabilitation technology requires considerable input from HCI researchers, and which explains the growing number of relevant HCI publications pertaining to rehabilitation.

The talk reviews related research carried out at the Eindhoven University of Technology together with collaborating institutes, which has examined the value of tangible user interfaces and embodied interaction in rehabilitation, how designing playful interactions or games with a functional purpose., feedback design. I shall discuss the work we have done to develop rehabilitation technologies for the TagTrrainer system in the doctoral research of Daniel Tetteroo [2,3,4] and the explorations on wearable solutions in the doctoral research of Wang Qi.[5,6]. With our research being design driven and explorative, I will discuss also the current state of the art for the field and the challenges that need to be addressed for human computer interaction research to make a larger impact in the domain of rehabilitation technology.</abstract><author_list>Panos Markopoulos</author_list></RECORD><RECORD><contentID>12590</contentID><title>Modern Touchscreen Keyboards as Intelligent User Interfaces: A Research Review</title><contentType>Keynote</contentType><presentationID>10436</presentationID><abstract>Essential to mobile communication, the touchscreen keyboard is the most ubiquitous intelligent user interface on modern mobile phones. Developing smarter, more efficient, easy to learn, and fun to use keyboards has presented many fascinating IUI research and design questions. Some have been addressed by academic research and practitioners in industry, while others remain significant ongoing research challenges.

In this IUI 2017 keynote address I will review and synthesize the progress and open research questions of the past 15 years in text input, focusing on those my colleagues and I have directly dealt with through publications, such as the cost&#x2013;benefit equations of automation and prediction, the power of machine/statistical intelligence, the human performance models fundamental to the design of error-correction algorithms, spatial scaling from a phone to a watch and the implications on human&#x2013;machine labor division, user behavior and learning innovation, and the challenges of evaluating the longitudinal effects of personalization and adaptation.

Through this research program review, I will illustrate why intelligent user interfaces, or the combination of machine intelligence and human factors, holds the future of human-computer interaction, and information technology at large.</abstract><author_list>Shumin Zhai</author_list></RECORD><RECORD><contentID>12591</contentID><title>Utilizing Human Cognitive and Emotional Factors for User-Centered Computing</title><contentType>Keynote</contentType><presentationID>10476</presentationID><abstract>Intelligent interactive systems should not ignore the individuality of the user. The "one-size-fits-all" approach, especially in user interaction, is not appropriate when user satisfaction and acceptability is a primary goal. Each user has unique human cognitive processing styles and abilities. In addition, emotions change over time, which possibly affect the user's cognitive state and the overall interaction process. Unsurprisingly, the users' ability to control their emotions is another essential factor in adapting user interfaces, applications and data delivery.

How can an interactive system adapt to human cognitive and emotional factors with the aim to deliver a personalized and more usable interface? Is there a user interface to an application or system that is equally effective to all types of users? How can we place the human in the center of every day's interaction and task activity?

This keynote speech will present some approaches that our work at the DMAC Lab/SCRAT Group has addressed on how individual differences in human cognitive processing and emotional factors place the user in the center of every day interaction.</abstract><author_list>George Samaras</author_list></RECORD><RECORD><contentID>12497</contentID><title>"How May I Help You?": Modeling Twitter Customer Service Conversations Using Fine-Grained Dialogue Acts</title><contentType>Long Paper</contentType><presentationID>10493</presentationID><abstract>Given the increasing popularity of customer service dialogue on Twitter, analysis of conversation data is essential to understand trends in customer and agent behavior for the purpose of automating customer service interactions. In this work, we develop a novel taxonomy of fine-grained \</abstract><author_list>Shereen Oraby, Pritam Gundecha, Jalal Mahmud, Mansurul Bhuiyan, Rama Akkiraju</author_list></RECORD><RECORD><contentID>12495</contentID><title>A 3D Item Space Visualization for Presenting and Manipulating User Preferences in Collaborative Filtering</title><contentType>Long Paper</contentType><presentationID>10437</presentationID><abstract>While conventional Recommender Systems perform well in automatically generating personalized suggestions, it is often difficult for users to understand why certain items are recommended and which parts of the item space are covered by the recommendations. Also, the available means to influence the process of generating results are usually very limited. To alleviate these problems, we suggest a 3D map-based visualization of the entire item space in which we position and present sample items along with recommendations. The map is produced by mapping latent factors obtained from Collaborative Filtering data onto a 2D surface through Multidimensional Scaling. Then, areas that contain items relevant with respect to the current user\'s preferences are shown as elevations on the map, areas of low interest as valleys. In addition to the presentation of his or her preferences, the user may interactively manipulate the underlying profile by raising or lowering parts of the landscape, also at cold-start. Each change may lead to an immediate update of the recommendations. Using a demonstrator, we conducted a user study that, among others, yielded promising results regarding the usefulness of our approach.</abstract><author_list>Johannes Kunkel, Benedikt Loepp, J&#xFC;rgen Ziegler</author_list></RECORD><RECORD><contentID>12530</contentID><title>A Data-Driven, Multidimensional Approach to Hint Design in Video Games</title><contentType>Long Paper</contentType><presentationID>10451</presentationID><abstract>Hint systems are designed to adjust a video game\'s difficulty to suit the individual player, but too often they are designed without analyzing player behavior and lack intelligence and adaptability, resulting in hints that are at best ineffective and at worst hurt player experience. We present an alternative approach to hint design focusing on player experience rather than performance. We had 25 participants play a difficult spatial puzzle game and collected player behavior, demographics, and self-reported player experience measures. We found that more exploratory behavior improved player experience, so we designed three types of hints encouraging this behavior: adaptive, automatic, and on-demand. We found that certain players found hints more helpful regardless of whether the hints changed their behavior, and players seemed to prefer seeing fewer hints than the adaptive and automatic conditions gave them. Our findings contribute a deeper empirical understanding of hint design strategies and their effect on player behavior and experience, with practical recommendations for designers of interactive systems.</abstract><author_list>Helen Wauck, Wai-Tat Fu</author_list></RECORD><RECORD><contentID>12483</contentID><title>A Network-Fusion Guided Dashboard Interface for Task-Centric Document Curation</title><contentType>Long Paper</contentType><presentationID>10507</presentationID><abstract>Knowledge workers are being exposed to more information than ever before, as well as having to work in multi-tasking and collaborative environments. There is an increasing need for interfaces and algorithms to help automatically keep track of documents that are associated with both individual and team tasks. Previous approaches to the problem of automatically applying task labels to documents have been limited to small feature spaces or have not taken into account multi-user environments. Many different clues to potential task associations are available through user, task and document similarity metrics, as well as through temporal patterns in individual and team workflows.     We present a network-fusion algorithm for automatic task-centric document curation, and show how this can guide a recent-work dashboard interface, which organizes user\'s documents and gathers feedback from them. Our approach efficiently computes representations of users, tasks and documents in a common vector space, and can easily take into account many different types of associations through the creation of edges in a multi-layer graph.    We have demonstrated the effectiveness of this approach using labelled document corpora from three empirical studies with students and intelligence analysts. We have also shown how to leverage relationships between different entity types to increase classification accuracy by up to 20% over a simpler baseline, and with as little as 10% labelled data.</abstract><author_list>Paul Jones, Changsung Moon, Shivani Sharma, Nagiza Samatova</author_list></RECORD><RECORD><contentID>12485</contentID><title>Adaptive View Management for Drone Teleoperation in Complex 3D Structures</title><contentType>Long Paper</contentType><presentationID>10501</presentationID><abstract>Drone navigation in complex environments poses many problems to teleoperators. Especially in 3D structures like buildings or tunnels, viewpoints are often limited to the drone\'s current camera view, nearby objects can be collision hazards, and frequent occlusion can hinder accurate manipulation.     To address these issues, we have developed a novel interface for teleoperation that provides a user with environment-adaptive viewpoints that are automatically configured to improve safety and smooth user operation. This real-time adaptive viewpoint system takes robot position, orientation, and 3D pointcloud information into account to modify user-viewpoint to maximize visibility. Our prototype uses simultaneous localization and mapping (SLAM) based reconstruction with an omnidirectional camera and we use resulting models as well as simulations in a series of preliminary experiments testing navigation of various structures. Results suggest that automatic viewpoint generation can outperform first and third-person view interfaces for virtual teleoperators in terms of ease of control and accuracy of robot operation.</abstract><author_list>John Thomason, Photchara Ratsamee, Kiyoshi Kiyokawa, Pakpoom Kriangkomol, Jason Orlosky, Tomohiro Mashita, Yuki Uranishi, Haruo Takemura</author_list></RECORD><RECORD><contentID>12533</contentID><title>Analyza: Exploring Data with Conversation</title><contentType>Long Paper</contentType><presentationID>10508</presentationID><abstract>We describe Analyza, a system that helps lay users explore  data. Analyza has been used within two large real world systems. The  first is a question-and-answer feature in a spreadsheet product. The  second provides convenient access to a revenue/inventory database  for a large sales force. Both user bases consist of users who do not  necessarily have coding skills, demonstrating Analyza\'s ability to  democratize access to data.    We discuss the key design decisions in implementing this system.  For instance, how to mix structured and natural language modalities,  how to use conversation to disambiguate and simplify querying, how  to rely on the ``semantics\'\' of the data to compensate for the lack  of syntactic structure, and how to efficiently curate the data. </abstract><author_list>Kedar Dhamdhere, Kevin McCurley, Ralfi Nahmias, Mukund Sundararajan, Qiqi Yan</author_list></RECORD><RECORD><contentID>12517</contentID><title>BoostFM: Boosted Factorization Machines for Top-N Feature-based Recommendation</title><contentType>Long Paper</contentType><presentationID>10441</presentationID><abstract>Feature-based matrix factorization techniques such as Factorization Machines (FM) have been proven to achieve impressive  accuracy for the rating prediction task. However, most common recommendation scenarios are formulated as a top-N item ranking problem with implicit feedback (e.g., clicks, purchases)rather than explicit ratings. To address this problem, with both implicit feedback and feature information, we propose a feature-based collaborative boosting recommender called BoostFM, which integrates boosting into factorization models during the process of item ranking. Specifically, BoostFM is an adaptive boosting framework that linearly combines multiple homogeneous component recommenders, which are repeatedly constructed on the basis of the individual FM model by a re-weighting scheme. Two ways are proposed to efficiently train the component recommenders from the perspectives of both pairwise and listwise Learning-to-Rank (L2R). The properties of our proposed method are empirically studied on three real-world datasets. The experimental results show that BoostFM outperforms a number of state-of-the-art approaches for top-N recommendation.</abstract><author_list>Fajie Yuan, Guibing Guo, Joemon Jose, Long Cheng, Haitao Yu, Weinan Zhang</author_list></RECORD><RECORD><contentID>12520</contentID><title>CarNote: Reducing Misunderstanding between Drivers by Digital Augmentation</title><contentType>Long Paper</contentType><presentationID>10445</presentationID><abstract>The road environment can be seen as a social situation: Drivers need to coordinate with each other to share the infrastructure. In addition to the driving behaviour itself, lights, horn and speed are the most frequently used means to exchange information, limiting both the range and the bandwidth of the connectivity and leading to misunderstanding and conflict. With everywhere available connectivity and the broad penetration of social network services, the relationship between drivers on the road may gain more transparency, enabling social information to pass through the steel shell of the cars and giving opportunities to reduce misunderstanding and strengthen empathy. In this study, we present &#xBA;CarNote&#xB9;, a concept that aims to reduce misunderstanding and conflict between drivers by showing their emergency driving status to others. This concept was prototyped and evaluated with users in a driving simulator. The results showed that CarNote enhances drivers&#xB4; empathy, increases forgiveness and decreases anger to others on the road. </abstract><author_list>Chao Wang, Jacques Terken, Jun Hu</author_list></RECORD><RECORD><contentID>12539</contentID><title>Cartograph: Unlocking Spatial Visualization Through Semantic Enhancement</title><contentType>Long Paper</contentType><presentationID>10455</presentationID><abstract>This paper introduces Cartograph, a visualization system that harnesses the vast amount of world knowledge encoded within Wikipedia to create thematic maps of almost any data. Cartograph extends previous systems that visualize non-spatial data using geographic approaches. While these systems required data with an existing semantic structure, Cartograph unlocks spatial visualization for a much larger variety of datasets by enhancing input datasets with semantic information extracted from Wikipedia. Cartograph\'s map embeddings use neural networks trained on Wikipedia article content and user navigation behavior. Using these embeddings, the system can reveal connections between points that are unrelated in the original data sets, but are related in meaning and therefore embedded close together on the map. We describe the design of the system and key challenges we encountered, and we present findings from an exploratory user study</abstract><author_list>Shilad Sen, Anja Beth Swoap, Qisheng Li, Brooke Boatman, Ilse Dippenaar, Rebecca Gold, Monica Ngo, Sarah Pujol, Bret Jackson, Brent Hecht</author_list></RECORD><RECORD><contentID>12519</contentID><title>CogniLearn: A Deep Learning-based Interface for Cognitive Behavior Assessment</title><contentType>Long Paper</contentType><presentationID>10522</presentationID><abstract>This paper proposes a novel system for assessing physical exercises specifically designed for cognitive behavior monitoring. The proposed system provides decision support to experts for helping with early childhood development. Our work is based on the well-established framework of Head-Toes-Knees-Shoulders (HTKS) that is known for its sufficient psychometric properties and its ability to assess cognitive dysfunctions. HTKS serves as a useful measure for behavioral self-regulation. Our system, CogniLearn, automates capturing and motion analysis of users performing the HTKS game and provides detailed evaluations using state-of-the-art computer vision and deep learning based techniques for activity recognition and evaluation. The proposed system is supported by an intuitive and specifically designed user interface that can help human experts to cross-validate and/or refine their diagnosis. To evaluate our system, we created a novel dataset, that we made open to the public to encourage further experimentation. The dataset consists of 15 subjects performing 4 different variations of the HTKS task and contains in total more than 60,000 RGB frames, of which 4,443 are fully annotated.</abstract><author_list>Srujana Gattupalli, Dylan Ebert, Michalis Papakostas, Fillia Makedon, Vassilis Athitsos</author_list></RECORD><RECORD><contentID>12512</contentID><title>Confiding in and Listening to Virtual Agents: The Effect of Personality</title><contentType>Long Paper</contentType><presentationID>10487</presentationID><abstract>We present an intelligent virtual interviewer that engages with a user in a text-based conversation and automatically infers the user&#xB4;s psychological traits, such as personality. We investigate how the personality of a virtual interviewer influences a user&#xB4;s behavior from two perspectives: the user&#xB4;s willingness to confide in, and listen to, a virtual interviewer. We have developed two virtual interviewers with distinct personalities and deployed them in a real-world recruiting event. We present findings from completed inter- views with 316 actual job applicants. Notably, users are more willing to confide in and listen to a virtual interviewer with a serious, assertive personality. Moreover, users&#xB4; personality traits, inferred from their chat text, influence their perception of a virtual interviewer, and their willingness to confide in and listen to a virtual interviewer. Finally, we discuss the implications of our work on building hyper- personalized, intelligent agents based on user traits.</abstract><author_list>Jingyi Li, Michelle Zhou, Huahai Yang, Gloria Mark</author_list></RECORD><RECORD><contentID>12516</contentID><title>CQAVis: Visual Text Analytics for Community Question Answering</title><contentType>Long Paper</contentType><presentationID>10453</presentationID><abstract>Community question answering (CQA) forums can provide effective means for sharing information and addressing a user\'s information needs about particular topics. However, many such online forums are not moderated, resulting in many low quality and redundant comments, which makes it very challenging for users to find the appropriate answers to their questions.  In this paper, we apply a user-centered design approach to develop a system, CQAVis, which supports users in identifying high quality comments and get their questions answered. Informed by the user\'s requirements, the system combines both text analytics and interactive visualization techniques together in a synergistic way. Given a new question posed by the user, the text analytic module automatically finds relevant answers  by exploring existing related questions and the  comments within their threads. Then the visualization module presents the search results to the user and supports the exploration of related comments. We have evaluated the system in the wild by deploying it within a  CQA forum among thousands of real users. Through the online study, we gained deeper insights about the potential utility of the system, as well as learned generalizable lessons for designing visual text analytics systems for the domain of CQA forums.</abstract><author_list>Enamul Hoque, Shafiq Joty, Luis Marquez, Giuseppe Carenini</author_list></RECORD><RECORD><contentID>12480</contentID><title>Dimensions for automatic interpretation of approximate numerical expressions: An empirical study</title><contentType>Long Paper</contentType><presentationID>10448</presentationID><abstract>Imprecise numerical expressions, such as about 100 meters, are pervasive in natural language. Mobile robotics, Geographic Information Systems, intelligent personal assistants as well as database querying applications are required to automatically and accurately interpret such expressions, called Approximate Numerical Expressions (ANE). The main challenge is to determine their numerical boundaries that sound plausible to users. The aim of this paper is to provide guidelines to interpret ANEs that are independent from the domain and the formal representations. We identified three arithmetical properties and examined their involvement in ANE interpretation as intervals of denoted values. The implicit assumption of symmetry of the intervals was also tested. To do so, 146 participants were asked to provide the intervals corresponding to 24 ANEs in a semantically neutral context. Results suggest that the properties of ANEs we identified are key factors in their interpretation while symmetry is not always maintained. This study contributes towards an understanding of how users process ANEs and its results can be used to improve intelligent interfaces that lead to better users satisfaction and natural interaction between him/her and the system.</abstract><author_list>S&#xE9;bastien Lefort, Elisabetta Zibetti, Marie-Jeanne Lesot, Marcin Detyniecki, Charles Tijus</author_list></RECORD><RECORD><contentID>12518</contentID><title>Don\'t Just Swipe Left, Tell Me Why: Enhancing Gesture-based Feedback with Reason Bins</title><contentType>Long Paper</contentType><presentationID>10506</presentationID><abstract>Despite several advances in information retrieval systems and user interfaces, the specification of queries over text-based document collections remains a challenging problem. Query specification with keywords is a popular solution. However, given the widespread adoption of gesture-driven interfaces such as multitouch technologies in smartphones and tablets, the lack of a physical keyboard makes query specification with keywords inconvenient. We present BinGO, a novel gestural approach to querying text databases that allows users to refine their queries using a swipe gesture to either ``like\'\' or ``dislike\'\' candidate documents as well as express the reasons they like or dislike a document by swiping through automatically generated ``reason bins\'\'. Such reasons refine a user\'s query with additional keywords. We present an online and efficient bin generation algorithm that presents reason bins at gesture articulation. We motivate and describe BinGo\'s unique interface design choices. Based on our analysis and user studies, we demonstrate that query specification by swiping through reason bins is easy and expressive.</abstract><author_list>Juan Felipe Beltran, Ziqi Huang, Azza Abouzied, Arnab Nandi</author_list></RECORD><RECORD><contentID>12505</contentID><title>Driver Readiness Model for Regulating the Transfer from Automation to Human Control</title><contentType>Long Paper</contentType><presentationID>10478</presentationID><abstract>In the collaborative driving scenario of truck platooning, the first car is driven by its chauffeur and the next cars follow automatically via a so-called `virtual tow-bar\'. The chauffeurs of the following cars do not drive `in the towbar mode\', but need to be able to take back control in foreseen emph{and} unforeseen conditions. It is crucial that this transfer of control only takes place when the chauffeur is ready for it. This paper presents a Driver Readiness (DR) ontological model that specifies the core factors, with their relationships, of a chauffeur\'s current and near-future readiness for taking back the control of driving.   A first model was derived from a literature study and an analysis of truck driving data, which was refined subsequently based on an expert review. This DR model distinguishes (a) current and required states for the physical (hand, feet, head, and seating position) and mental readiness (attention and situation awareness), (b) agents (human and machine actor), (c) policies for agent behaviors, and (d) states of the vehicle and its environment. It provides the knowledge base of a Control Transfer Support (CTS) agent that assesses the current and predicted chauffeur state and guides the transition of control in an adaptive and personalized manner. The DR model will be fed by information from the network and in-car sensors. The behaviors of the CTS agent will be generated and constrained by the instantiated policies, providing an important step towards a safe transfer of control from automation to human driver.</abstract><author_list>Tina Mioch, Liselotte Kroon, Mark Neerincx</author_list></RECORD><RECORD><contentID>12522</contentID><title>DyFAV: Dynamic Feature Selection and Voting for real-time recognition of fingerspelled alphabet using wearables</title><contentType>Long Paper</contentType><presentationID>10505</presentationID><abstract>Recent research has shown that reliable recognition of sign language words and phrases using user-friendly and non-invasive armbands is feasible and desirable. This work provides an analysis and implementation of including fingerspelling recognition (FR) in such systems, which is a much harder problem due to lack of distinctive hand movements. A novel algorithm called DyFAV (Dynamic Feature Selection and Voting) is proposed for this purpose that exploits the fact that fingerspelling has a finite corpus (26 letters for ASL). The system uses an independent multiple agent voting approach to identify letters with high accuracy. The independent voting of the agents  ensures that the algorithm is highly parallelizable and thus recognition times can be kept low to suit real-time mobile applications. The results are demonstrated on the entire ASL alphabet corpus for nine people with limited training and average recognition accuracy of 95.36% is achieved which is  better than the state-of-art for armband sensors. The mobile, non-invasive, and real time nature of the technology is demonstrated by evaluating performance on various types of Android phones and remote server configurations.</abstract><author_list>Prajwal Paudyal, Junghyo Lee, Ayan Banerjee, Sandeep Gupta</author_list></RECORD><RECORD><contentID>12540</contentID><title>Effect of Motion-Gesture Recognizer Error Pattern on User Workload and Behavior</title><contentType>Long Paper</contentType><presentationID>10503</presentationID><abstract>Bi-level thresholding is a motion gesture recognition technique that mediates between false positives, and false negatives by using two threshold levels:  a tighter threshold that limits false positives and recognition errors, and a looser threshold that prevents repeated errors (false negatives) by analyzing movements in sequence. In this paper, we examine the effects of bi-level thresholding on the workload and acceptance of end-users. Using a wizard-of-Oz recognizer, we hold recognition rates constant and adjust for fixed versus bi-level thresholding. Given identical recognition rates, we show that systems using bi-level thresholding result in significant lower workload scores on the NASA-TLX and accelerometer variance. Overall, these results argue for the viability of bi-level thresholding as an effective technique for balancing between false positives, recognition errors and false negatives.</abstract><author_list>Keiko Katsuragawa, Ankit Kamal, Edward Lank</author_list></RECORD><RECORD><contentID>12479</contentID><title>Explaining Recommendations Based on Feature Sentiments in Product Reviews</title><contentType>Long Paper</contentType><presentationID>10438</presentationID><abstract>The explanation interface has been recognized important in recommender systems as it can help users evaluate recommendations in a more informed way for deciding which ones are relevant to their interests. In different decision environments, the specific aim of explanation can be different. In high-investment product domains (e.g., digital cameras, laptops) for which users usually attempt to avoid financial risk, how to support users to construct stable preferences and make better decisions is particularly crucial. In this paper, we propose a novel explanation interface that emphasizes explaining the tradeoff properties within a set of recommendations in terms of both their static specifications and feature sentiments extracted from product reviews. The objective is to assist users in more effectively exploring and understanding product space, and being able to better formulate their preferences for products by learning from other customers\' experiences. Through two user studies (in form of both before-after and within-subjects experiments), we empirically identify the practical role of feature sentiments in combination with static specifications in producing tradeoff-oriented explanations. Specifically, we find that our explanation interface can be more effective to increase users\' product knowledge, preference certainty, perceived information usefulness, recommendation transparency and quality, and purchase intention.</abstract><author_list>Li Chen, Feng Wang</author_list></RECORD><RECORD><contentID>12481</contentID><title>Help, It Looks Confusing: GUI Task Automation Through Demonstration and Follow-up Questions</title><contentType>Long Paper</contentType><presentationID>10482</presentationID><abstract>Non-programming users should be able to create their own customized scripts to perform computer-based tasks for them, just by demonstrating to the machine how it\'s done. To that end, we develop a system prototype which learns-by-demonstration called HILC (Help, It Looks Confusing). Users train HILC to synthesize a task script by demonstrating the task, which produces the needed screenshots and their corresponding mouse-keyboard signals. After the demonstration, the user answers follow-up questions.    We propose a user-in-the-loop framework that learns to generate scripts of actions performed on visible elements of graphical applications. While pure programming-by-demonstration is still unrealistic, we use quantitative and qualitative experiments to show that non-programming users are willing and effective at answering follow-up queries posed by our system. Our models of events and appearance are surprisingly simple, but are combined effectively to cope with varying amounts of supervision.    The best available baseline, Sikuli Slides, struggled with the majority of the tests in our user study experiments. The prototype with our proposed approach successfully helped users accomplish simple linear tasks, complicated tasks (monitoring, looping, and mixed), and tasks that span across multiple executables. Even when both systems could ultimately perform a task, ours was trained and refined by the user in less time.</abstract><author_list>Thanapong Intharah, Daniyar Turmukhambetov, Gabriel Brostow</author_list></RECORD><RECORD><contentID>12515</contentID><title>How to Recommend? User Trust Factors in Movie Recommender Systems</title><contentType>Long Paper</contentType><presentationID>10488</presentationID><abstract>How much trust a user places in a recommender is crucial to the uptake of the recommendations. Although prior work established various factors that build and sustain user trust, their comparative impact has not been studied in depth. This paper presents the results of a crowdsourced study examining the impact of various recommendation interfaces and content selection strategies on user trust. It evaluates the subjective ranking of nine key factors of trust grouped into three dimensions and examines the differences observed with respect to users\' personality traits.</abstract><author_list>Shlomo Berkovsky, Ronnie Taib, Dan Conway</author_list></RECORD><RECORD><contentID>12490</contentID><title>Identifying Frequent User Tasks from Application Logs</title><contentType>Long Paper</contentType><presentationID>10485</presentationID><abstract>In the light of continuous growth in log analytics, application logs remain a valuable source to understand and analyze patterns in user behavior. Today, almost every major software company employs analysts to reveal user insights from log data. To understand the tasks and challenges of the analysts, we conducted a background study with a group of analysts from a major software company. A fundamental analytics objective that we recognized through this study involves identifying frequent user tasks from application logs. More specifically, analysts are interested in identifying operation groups that represent meaningful tasks performed by many users inside applications. This is challenging, primarily because of the nature of modern application logs, which are long, noisy and consist of events from high-cardinality set. In this paper, we address these challenges to design a novel frequent pattern ranking technique that extracts frequent user tasks from application logs. Our experimental study shows that our proposed technique significantly outperforms state of the art for real-world data.</abstract><author_list>Himel Dev, Zhicheng Liu</author_list></RECORD><RECORD><contentID>12534</contentID><title>Intelligent Sensory Modality Selection for Electronic Supportive Devices</title><contentType>Long Paper</contentType><presentationID>10442</presentationID><abstract>Humans operating in stressful environments, such as in military or emergency first-responder roles, are subject to high sensory input loads and must often switch their attention between different modalities. Conventional supportive devices that assist users in such situations typically provide information using a single, static sensory modality; however, this carries the risk of overload when the modalities for the primary task and the supportive device overlap. Effective feedback modality selection is essential in order to avoid such a risk. One potential method for accomplishing this is to intelligently select the supportive device\'s feedback modality based on the user\'s environment and given task; however, this may result in delayed or lost information due to the performance cost resulting from switching attention from one modality to another. This paper describes the design and results of a human-participant study designed to evaluate the benefits and risks of various intelligent modality-selection strategies. Our findings suggest complex interactions between strategies, sensory input load levels and feedback modalities, with numerous significant effects across many different performance metrics.</abstract><author_list>Kyle Kotowick, Julie Shah</author_list></RECORD><RECORD><contentID>12478</contentID><title>Interactive machine learning via a GPU-accelerated Toolkit</title><contentType>Long Paper</contentType><presentationID>10513</presentationID><abstract>Machine learning is growing in importance in industry, sciences, and many other fields. In many  and perhaps most of these applications, users need to trade off competing goals. Machine learning,  however, has evolved around the optimization of a single, usually narrowly-defined criterion. In most  cases, an expert makes (or should be making) trade-offs between these criteria which requires high-level (human)  intelligence. With interactive customization and optimization the expert can incorporate  secondary criteria into the model-generation process in an interactive way.     In this paper we develop the techniques to perform customized and interactive model optimization, and demonstrate  the approach on several examples. The keys to our approach are (i) a machine learning architecture which is  modular and supports primary and secondary loss functions, while users can directly manipulate its parameters during training (ii) high-performance training so that non-trivial models can be trained in real-time (using roofline design and GPU hardware), and (iii) highly-interactive visualization tools  that support dynamic creation of visualizations and controls to match various optimization criteria.</abstract><author_list>Biye Jiang, John Canny</author_list></RECORD><RECORD><contentID>12514</contentID><title>Label-and-Learn: Visualizing the Likelihood of Machine Learning Classifier&#xB4;s Success During Data Labeling</title><contentType>Long Paper</contentType><presentationID>10512</presentationID><abstract>While machine learning is a powerful tool for the analysis and classification of complex real-world datasets, it is still challenging, particularly for developers with limited expertise, to incorporate this technology into their software systems. The first step in machine learning, data labeling, is traditionally thought of as a tedious, unavoidable task in building a machine learning classifier. However, in this paper, we argue that it can also serve as the first opportunity for developers to gain insight into their dataset. Through a Label-and-Learn interface, we explore visualization strategies that leverage the data labeling task to enhance developers\' knowledge about their dataset, including the likely success of the classifier and the rationale behind the classifier\'s decisions. At the same time, we show that the visualizations also improve users\' labeling experience by showing them the impact they have made on classifier performance. We assess the visualizations in Label-and-Learn and experimentally demonstrate their value to software developers who seek to assess the utility of machine learning during the data labeling process.</abstract><author_list>Yunjia Sun, Edward Lank, Michael Terry</author_list></RECORD><RECORD><contentID>12502</contentID><title>Measuring Self-Esteem with Games</title><contentType>Long Paper</contentType><presentationID>10447</presentationID><abstract>Self-esteem is a personality trait utilized to support the diagnosis of several psychological conditions. With this study we investigate the potential that computer games can have in assessing self-esteem. To that end, we designed and developed a platformer game and analyzed how in-game behavior relates to Rosenberg\'s Self-Esteem Scale. We examined: i) how a player&#xB4;s self-esteem influences game performance, ii) how a player&#xB4;s self-esteem generally influences in-game behavior iii) the possible game mechanics that assist in inferring a player&#xB4;s self-esteem. The study was conducted in two phases (N=98 and N=85). Results indicate that self-esteem does not have any impact on the player&#xB4;s performance, on the other hand, we found that players&#xB4; self-evaluation of game performance correlates with their self-esteem.</abstract><author_list>Carlos Pereira Santos, Kevin Hutchinson, Vassilis-Javed Khan, Panos Markopoulos</author_list></RECORD><RECORD><contentID>12528</contentID><title>Negative Relevance Feedback for Exploratory Search with Visual Interactive Intent Modeling</title><contentType>Long Paper</contentType><presentationID>10452</presentationID><abstract>In difficult information seeking tasks, the majority of top-ranked documents for an initial query may be non-relevant, and negative relevance feedback may then help find relevant documents. Traditional negative relevance feedback has been studied on document results; we introduce a system and interface for negative feedback in a novel exploratory search setting, where continuous-valued feedback is directly given to keyword features of an inferred probabilistic user intent model. The introduced system allows both positive and negative feedback directly on an interactive visual interface, by letting the user manipulate keywords on an optimized visualization of modeled user intent. Feedback on the interactive intent model lets the user direct the search: Relevance of keywords is estimated from feedback by Bayesian inference, influence of feedback is increased by a novel propagation step, documents are retrieved by likelihoods of relevant versus non-relevant intents, and the most relevant keywords (having the highest upper confidence bounds of relevance) and the most non-relevant ones (having the smallest lower confidence bounds of relevance) are shown as options for further feedback. We carry out task-based information seeking experiments with real users on difficult real tasks; we compare the system to the nearest state of the art baseline allowing positive feedback only, and show negative feedback significantly improves the quality of retrieved information and user satisfaction for difficult tasks.</abstract><author_list>Jaakko Peltonen, Jonathan Strahl, Patrik Flor&#xE9;en</author_list></RECORD><RECORD><contentID>12486</contentID><title>Novelty Learning via Collaborative Proximity Filtering</title><contentType>Long Paper</contentType><presentationID>10517</presentationID><abstract>The vast majority of recommender systems model preferences as static or slowly changing due to observable user experience. However, spontaneous changes in user preferences are ubiquitous in many domains like media consumption and key factors that drive changes in preferences are not directly observable. These latent sources of preference change pose new challenges. When systems do not track and adapt to users\' tastes, users lose confidence and trust, increasing the risk of user churn. We meet these challenges by developing a model of novelty preferences that learns and tracks latent user tastes. We combine three innovations: a new measure of item similarity based on patterns of consumption co-occurrence; model for spontaneous changes in preferences; and a learning agent that tracks each user\'s dynamic preferences and learns individualized policies for variety. The resulting framework adaptively provides users with novelty tailored to their preferences for change per se.</abstract><author_list>Arun Kumar, Paul Schrater</author_list></RECORD><RECORD><contentID>12488</contentID><title>Once More, With Feeling: Expressing Emotional Intensity in Touchscreen Gestures</title><contentType>Long Paper</contentType><presentationID>10502</presentationID><abstract>In this paper, we explore how people use touchscreens to express emotional intensity, and whether these intensities can be understood by oneself at a later date or by others. In a controlled study, 26 participants were asked to express a set of emotions mapped to predefined gestures, at range of different intensities. One week later, participants were asked to identify the emotional intensity visualized in animations of the gestures made by themselves and by other participants. Our participants expressed emotional intensity using gesture length, pressure, and speed primarily; the choice of attributes was impacted by the specific emotion, and the range and rate of increase of these attributes varied by individual and by emotion. Recognition accuracy of emotional intensity was higher at extreme ends, and was higher for one&#xB4;s own gestures than those made by others. The attributes of size and pressure (mapped to color in the animation) were most readily interpreted, while speed was more difficult to differentiate. We discuss human gesture drawing patterns to express emotional intensities and implications for developers of annotation systems and other touchscreen interfaces that wish to capture affect.</abstract><author_list>Nabil Bin Hannan, Khalid Tearo, Joseph Malloch, Derek Reilly</author_list></RECORD><RECORD><contentID>12508</contentID><title>OptiDwell: Intelligent Adjustment of Dwell Click Time</title><contentType>Long Paper</contentType><presentationID>10477</presentationID><abstract>Gaze based navigation with digital screens offer a hands-free and touchless interaction, which is often useful in providing a hygienic interaction experience in a public kiosk scenario.  The goodness of such a navigation system depends not only on the accuracy of detecting the eye gaze but also on the ability to determine whether a user is interested in clicking a button or is just looking at the button.  The time for which a user needs to gaze at a particular button before it is considered as a click action is called the dwell time.  In this paper, we explore intelligent adjustment of dwell times, where mouse click events on the buttons of a given application are emulated with user gaze. A constant dwell-time for all buttons and for all users may not provide an efficient and intuitive interface.  We thereby propose a model to dynamically adjust dwell-time values used to emulate user mouse click events, exploiting the user\'s experience with different portions of a given application.  The adjustment happens at a per-user, per-button granularity, as a function of the user\'s (a) prior usage experience of the given button within the application and (b) Midas touch characteristics for the given button.    We propose OptiDwell, inspired by the action-value method based solutions to the Multi-Armed Bandits problem, for dwell click time adaptation.  We experiment OptiDwell using an interactive TV channel browsing interface application, constituting of a mix of text and image buttons, over 10 computer-savvy users generating over 9000 click tasks.   We observe significant improvement of user comfort level over the sessions, quantified by (a) improved (reduced) dwell times and (b) reduced number of Midas touches in spite of faster dwell-clicks, as high as 10-fold reduction in the best case.  Our work is useful for creating an interface, with accurate, fast and comfortable dwell-clicks for each interface element (e.g., buttons), and each user.</abstract><author_list>Aanand Nayyar, Utkarsh Dwivedi, Karan Ahuja, Nitendra Rajput, Seema Nagar, Kuntal Dey</author_list></RECORD><RECORD><contentID>12493</contentID><title>Pupillometry and Head Distance to the Screen to Predict Skill Acquisition During Information Visualization Tasks</title><contentType>Long Paper</contentType><presentationID>10480</presentationID><abstract>In this paper we investigate using a variety of behavioral measures collectible with an eye tracker to predict a user&#xB4;s skill acquisition phase while performing various information visualization tasks with bar graphs. Our long term goal is to use this information in real-time to create user-adaptive visualizations that can provide personalized support to facilitate visualization processing based on the user&#xB4;s predicted skill level. We show that leveraging two additional content-independent data sources, namely information on a user&#xB4;s pupil dilation and head distance to the screen, yields a significant improvement for predictive accuracies of skill acquisition compared to predictions made using content-dependent information related to user eye gaze attention patterns, as was done in previous work. We show that including features from both pupil dilation and head distance to the screen improve the ability to predict users&#xB4; skill acquisition state, beating both the baseline and a model using only content-dependent gaze information.</abstract><author_list>Dereck Toker, Sebastien Lalle, Cristina Conati</author_list></RECORD><RECORD><contentID>12510</contentID><title>Scaling Reflection Prompts in Large Classrooms via Mobile Interfaces and Natural Language Processing</title><contentType>Long Paper</contentType><presentationID>10495</presentationID><abstract>We present the iterative design, prototype, and evaluation of CourseMIRROR (Mobile In-situ Reflections and Review with Optimized Rubrics), an intelligent mobile learning system that uses natural language processing (NLP) techniques to enhance instructor-student interactions in large classrooms. CourseMIRROR enables streamlined and scaffolded reflection prompts by: 1) reminding and collecting students&#xB4; in-situ written reflections after each lecture; 2) continuously monitoring the quality of a student&#xB4;s reflection at composition time and generating helpful feedback to scaffold reflection writing; and 3) summarizing the reflections and presenting the most significant ones to both instructors and students. Through a combination of a 60-participant lab study and eight semester-long deployments involving 317 students, we found that the reflection and feedback cycle enabled by CourseMIRROR is beneficial to both instructors and students. Furthermore, the reflection quality feedback feature can encourage students to compose more specific and higher-quality reflections, and the algorithms in CourseMIRROR are both robust to cold start and scalable to STEM courses in diverse topics.</abstract><author_list>Xiangmin Fan, Wencan Luo, Muhsin Menekse, Diane Litman, Jingtao Wang</author_list></RECORD><RECORD><contentID>12491</contentID><title>SIRUP: Serendipity In Recommendations via User Perceptions</title><contentType>Long Paper</contentType><presentationID>10440</presentationID><abstract>In this paper, we propose a model to operationalise serendipity in content-based recommender systems. The model, called SIRUP, is inspired by the Silvia\'s curiosity theory, based on the fundamental theory of Berlyne, aims at (1) measuring the novelty of an item with respect to the user profile, and (2) assessing whether the user is able to manage such level of novelty (coping potential). The novelty of items is calculated with cosine similarities between items, using Linked Open Data paths. The coping potential of users is estimated by measuring the diversity of the items in the user profile. We deployed and evaluated the SIRUP model in a use case with TV recommender using BBC programs dataset. Results show that the SIRUP model allows us to identify serendipitous recommendations, and, at the same time, to have 71% precision.  </abstract><author_list>Valentina Maccatrozzo, Manon Terstall, Lora Aroyo, Guus Schreiber</author_list></RECORD><RECORD><contentID>12501</contentID><title>Social Intelligence Modeling using Wearable Devices</title><contentType>Long Paper</contentType><presentationID>10492</presentationID><abstract>Social Signal Processing techniques have given the opportunity to analyze in-depth human behavior in social face-to-face interactions. With recent advancements, it is henceforth possible to use these techniques to augment social interactions, especially the human behavior in oral presentations. The goal of this paper is to train a computational model able to provide a relevant feedback to a public speaker concerning his coverbal communication. Hence, the role of this model is to augment the social intelligence of the orator and then the relevance of his presentation. To this end, we present an original interaction setting in which the speaker is equipped with only wearable devices. Several coverbal modalities have been extracted and automatically annotated namely speech volume, intonation, speech rate, eye gaze, hand gestures and body movements. An offline report was addressed to participants containing the performance scores on the overall modalities. In addition, a post-experiment study was conducted to collect participant&#xB4;s opinions on many aspects of the studied interaction and the results were rather positive. Moreover, we annotated recommended feedbacks for each presentation session, and to retrieve these annotations, a Dynamic Bayesian Network model was trained using as inputs the multimodal performance scores. We will show that our assessment behavior model presents good performances compared to other models.</abstract><author_list>Alaeddine Mihoub, Gr&#xE9;goire Lefebvre</author_list></RECORD><RECORD><contentID>12504</contentID><title>SupportingTrust in Autonomous Driving</title><contentType>Long Paper</contentType><presentationID>10491</presentationID><abstract>Autonomous cars will likely hit the market soon, but trust into such a technology is one of the big discussion points in the public debate. Drivers who have always been in complete control of their car are expected to willingly hand over control and blindly trust a technology that could kill them.     We argue that trust in autonomous driving can be increased by means of a driver interface that visualizes the car\'s interpretation of the current situation and its corresponding actions. To verify this, we compared different visualizations in a user study, overlaid to a driving scene: (1) a chauffeur avatar, (2) a world in miniature, and (3) a display of the car\'s indicators as the baseline. The world in miniature visualization increased trust the most. The human-like chauffeur avatar can also increase trust, however, we did not find a significant difference between the chauffeur and the baseline. </abstract><author_list>Renate H&#xE4;uslschmid, Max von Buelow, Bastian Pfleging, Andreas Butz</author_list></RECORD><RECORD><contentID>12529</contentID><title>Topic-Relevance Map: Visualization for Improving Search Result Comprehension</title><contentType>Long Paper</contentType><presentationID>10518</presentationID><abstract>We introduce topic-relevance map, an interactive search result visualization that assists rapid information comprehension across a large ranked set of results. The topic-relevance map visualizes a topical overview of the search result space as keywords with respect to two essential information retrieval measures: relevance and topical similarity. Non-linear dimensionality reduction is used to embed high-dimensional keyword representations of search result data into angles on a radial layout. Relevance of keywords is estimated by a ranking method and visualized as radiuses on the radial layout. As a result, similar keywords are modeled by nearby points, dissimilar keywords are modeled by distant points, more relevant keywords are closer to the center of the radial display, and less relevant keywords are distant from the center of the radial display. We evaluated the effect of the topic-relevance map in a search result comprehension task where 24 participants were summarizing search results and produced a conceptualization of the result space. The results show that topic-relevance map significantly improves participants\' comprehension capability compared to a conventional ranked list presentation.</abstract><author_list>Jaakko Peltonen, Kseniia Belorustceva, Tuukka Ruotsalo</author_list></RECORD><RECORD><contentID>12499</contentID><title>Towards Automating Data Narratives</title><contentType>Long Paper</contentType><presentationID>10521</presentationID><abstract>We propose a new area of research on automating data narratives. Data narratives are containers of information about computationally generated research findings. They have three major components: 1) A record of events, that describe a new result through a workflow and/or provenance of all the computations executed; 2) Persistent entries for key entities involved for data, software versions, and workflows; 3) A set of narrative accounts that are automatically generated human-consumable renderings of the record and entities and can be included in a paper. Different narrative accounts can be used for different audiences with different content and details, based on the level of interest or expertise of the reader. Data narratives can make science more transparent and reproducible, because they ensure that the text description of the computational experiment reflects with high fidelity what was actually done. Data narratives can be incorporated in papers, either in the methods section or as supplementary materials. We introduce DANA, a prototype that illustrates how to generate data narratives automatically, and describe the information it uses from the computational records. We also present a formative evaluation of our approach and discuss potential uses of automated data narratives.</abstract><author_list>Yolanda Gil, Daniel Garijo</author_list></RECORD><RECORD><contentID>12482</contentID><title>Towards Future Interactive Intelligent Systems for Animals: Study and Recognition of Embodied Interactions</title><contentType>Long Paper</contentType><presentationID>10498</presentationID><abstract>User-centered design applied to non-human animals is showing to be a promising research line known as Animal Computer Interaction (ACI), aimed at improving animals&#xB4; wellbeing using technology. Within this research line, intelligent systems for animal entertainment could have remarkable benefits for their mental and physical wellbeing, while providing new ways of communication and amusement between humans and animals. In order to create user-centered interactive intelligent systems for animals, we first need to understand how they spontaneously interact with technology, and develop suitable mechanisms to adapt to the animals&#xB4; observed interactions and preferences. Therefore, this paper describes a pioneer study on cats&#xB4; preferences and behaviors with different technological devices. It also presents the design and evaluation of a promising depth-based tracking system for the detection of cats&#xB4; body parts and postures. The contributions of this work lay foundations towards providing a framework for the development of future intelligent systems for animal entertainment.</abstract><author_list>Patricia Pons, Javier Jaen, Alejandro Catala</author_list></RECORD><RECORD><contentID>12496</contentID><title>UI X-Ray: Interactive Mobile UI Testing Based on Computer Vision</title><contentType>Long Paper</contentType><presentationID>10483</presentationID><abstract>User Interface/eXperience (UI/UX) significantly affects the lifetime of any software program, particularly mobile apps. A bad UX can undermine the success of a mobile app even if that app enables sophisticated capabilities. A good UX, however, needs to be supported of a highly functional and user friendly UI design. In spite of the importance of building mobile apps based on solid UI designs, UI discrepancies---inconsistencies between UI design and implementation---are among the most numerous and expensive defects encountered during testing. This paper presents UI X-Ray, an interactive UI testing system that integrates computer-vision methods to facilitate the correction of UI discrepancies---such as inconsistent positions, sizes and colors of objects and fonts. Using UI X-Ray does not require any programming experience; therefore, UI X-Ray can be used even by non-programmers---particularly designers---which significantly reduces the overhead involved in writing tests.  With the feature of interactive interface, UI testers can quickly generate defect reports and revision instructions---which would otherwise be done manually.  We verified our UI X-Ray on 4 developed mobile apps of which the entire development history was saved.  UI X-Ray achieved a 99.03% true-positive rate, which significantly surpassed the 20.92% true-positive rate obtained via manual analysis.  Furthermore, evaluating the results of our automated analysis can be completed quickly (&lt; 1 minute per view on average) compared to hours of manual work required by UI testers.  On the other hand, UI X-Ray received the appreciations from skilled designers and UI X-Ray improves their current work flow to generate UI defect reports and revision instructions.  The proposed system, UI X-Ray, presented in this paper has recently become part of a commercial product.</abstract><author_list>Chun-Fu Chen, Marco Pistoia, Conglei Shi, Paolo Girolami, Joseph Ligman, Yong Wang</author_list></RECORD><RECORD><contentID>12492</contentID><title>Understanding Emotional Responses to Mobile Video Advertisements via Physiological Signal Sensing and Facial Expression Analysis</title><contentType>Long Paper</contentType><presentationID>10443</presentationID><abstract>Understanding a target audience&#xB4;s emotional responses to video advertisements is crucial to stakeholders. However, traditional methods for collecting such information are slow, expensive, and coarse-grained. We propose AttentiveVideo, an intelligent mobile interface with corresponding inference algorithms to monitor and quantify the effects of mobile video advertising. AttentiveVideo employs a combination of implicit photoplethysmography (PPG) sensing and facial expression analysis (FEA) to predict viewers&#xB4; attention, engagement, and sentiment when watching video advertisements on unmodified smartphones. In a 24-participant study, we found that AttentiveVideo achieved good accuracies on a wide range of emotional measures (the best average accuracy = 73.59%, kappa = 0.46 across 9 metrics). We also found that the PPG sensing channel and the FEA technique are complimentary. While FEA works better for strong emotions (e.g., joy and anger), the PPG channel is more informative for subtle responses or emotions. These findings show the potential for both low-cost collection and deep understanding of emotional responses to mobile video advertisements.</abstract><author_list>Phuong Pham, Jingtao Wang</author_list></RECORD><RECORD><contentID>12531</contentID><title>Untangling the Relationship Between Spatial Skills, Game Features, and Gender in a Video Game</title><contentType>Long Paper</contentType><presentationID>10450</presentationID><abstract>Certain commercial video games, such as Portal 2 and Tetris, have been empirically shown to train spatial reasoning skills, a subset of cognitive skills essential for success in STEM disciplines. However, no research to date has attempted to understand which specific features in these games tap into players\' spatial ability or how individual player differences interact with these game features. This knowledge is crucially important as a first step towards understanding what makes these games effective and why, especially for subpopulations with lower spatial ability such as women and girls. We present the first empirical study analyzing the relationship between spatial ability, specific game features, and individual player differences using a custom-built computer game. Twenty children took a pretest of spatial skills and then played our game for 2 hours. We found that spatial ability pretest scores predicted several player behaviors related to in-game tasks involving 3D object construction and first person navigation. However, when analyzed by gender, girls\' pretest scores were much less predictive of player behavior.</abstract><author_list>Helen Wauck, Ziang Xiao, Po-Tsung Chiu, Wai-Tat Fu</author_list></RECORD><RECORD><contentID>12525</contentID><title>User Trust Dynamics: An Investigation Driven by Differences in System Performance</title><contentType>Long Paper</contentType><presentationID>10490</presentationID><abstract>Trust is a key factor affecting the way people rely on automated systems. On the other hand, system performance has comprehensive implications on a user&#xB4;s trust variations. This paper examines systems of varied levels of accuracy, in order to reveal the relationship between system performance, a user&#xB4;s trust and reliance on the system. In particular, it is identified that system failures have a stronger effect on trust than system successes. We also describe how patterns of trust change according to a number of consecutive system failures or successes. Importantly, we show that increasing user familiarity with the system decreases the rate of trust change, which provides new insights on the development of user trust. Finally, our analysis established a correlation between a user&#xB4;s reliance on a system and their trust level. Combining all these findings can have important implications in general system design and implementation, by predicting how trust builds and when it stabilizes, as well as allowing for indirectly reading a user&#xB4;s trust in real time based on system reliance.</abstract><author_list>Kun Yu, Shlomo Berkovsky, Ronnie Taib, Dan Conway, Jianlong Zhou, Fang Chen</author_list></RECORD><RECORD><contentID>12535</contentID><title>Web Screen Reading Automation Assistance Using Semantic Abstraction</title><contentType>Long Paper</contentType><presentationID>10500</presentationID><abstract>A screen reader\'s sequential press-and-listen interface makes for an unsatisfactory and often times painful web-browsing experience for blind people. To help alleviate this situation, we introduce Web Screen Reading Automation Assistant (SRAA) for automating users&#xB4; screen-reading actions (e.g., finding price of an item) on demand, thereby letting them focus on what they want to do rather than on how to get it done. The key idea is to elevate the interaction from operating on (syntactic) HTML elements, as is done now, to operating on web entities (which are semantically meaningful collections of related HTML elements, e.g., search results, menus, widgets, etc.). SRAA realizes this idea of semantic abstraction by constructing a Web Entity Model (WEM), which is a collection of web entities of the underlying webpage, using an extensive generic library of custom-designed descriptions of commonly occurring web entities across websites. The WEM brings blind users closer to how sighted people perceive and operate on web entities, and together with a natural-language user interface, SRAA relieves users from having to press numerous shortcuts to operate on low-level HTML elements - the principal source of tedium and frustration. This paper describes the design and implementation of SRAA. Evaluation with 18 blind subjects demonstrates its usability and effectiveness.</abstract><author_list>Vikas Ashok, Yury Puzis, Yevgen Borodin, I.V. Ramakrishnan</author_list></RECORD><RECORD><contentID>12507</contentID><title>WikiLyzer: Interactive Information Quality Assessment in Wikipedia</title><contentType>Long Paper</contentType><presentationID>10497</presentationID><abstract>Digital libraries and services enable users to access large amounts of data on demand. Yet, quality assessment of information encountered on the Internet remains an elusive open issue. For example, Wikipedia, one of the most visited platforms on the Web, hosts thousands of user-generated articles and undergoes 12 million edits/contributions per month. User-generated content is undoubtedly one of the keys to its success, but also a hindrance to good quality: contributions can be of poor quality because anyone, even anonymous users, can participate. Though Wikipedia has defined guidelines as to what makes the perfect article, authors find it difficult to assert whether their contributions comply with them and reviewers cannot cope with the ever growing amount of articles pending review. Great efforts have been invested in algorithmic methods for automatic classification of Wikipedia articles (as featured or non-featured) and for quality flaw detection. However, little has been done to support quality assessment of user-generated content through interactive tools that combine automatic methods and human intelligence. We developed WikiLyzer, a Web toolkit comprising three interactive applications designed to assist (i) knowledge discovery experts in creating and testing metrics for quality measurement, (ii) Wikipedia users searching for good articles, and (iii) Wikipedia authors that need to identify weaknesses to improve a particular article. A design study sheds a light on how experts could create complex quality metrics with our tool, while a user study reports on its usefulness to identify high-quality content.</abstract><author_list>Cecilia di Sciascio, David Strohmaier, Marcelo Errecalde, Eduardo Veas</author_list></RECORD><RECORD><contentID>12561</contentID><title>A Gestural Interface for Practicing Children\'s Spatial Skills</title><contentType>Poster</contentType><presentationID>10457</presentationID><abstract>We present a novel gestural interface for an educational 3D construction game that helps children practice and learn spatial reasoning skills. Previous research shows that having well-developed spatial reasoning skills is crucial to the success in the STEM fields. Our game requires the player to create a number of 3D target objects by moving, rotating, and assembling smaller pieces in the right way through a gestural interface. The gestures were designed with enhancing the user experience and effectiveness of learning in mind, by having a congruent mapping between hand gestures and spatial operations. The initial results of the preliminary study we conducted with the children show that the interface has the potential to be used for practicing spatial reasoning skills with the game. We also discuss how the study can lead to the development of a theoretical framework for designing gestural interfaces for educational games that leverage the benefit of embodied interactions.</abstract><author_list>Yuan Yao, Po-Tsung Chiu, Wai-Tat Fu</author_list></RECORD><RECORD><contentID>12562</contentID><title>An Analysis and Modeling Framework for Personalized Interaction</title><contentType>Poster</contentType><presentationID>10458</presentationID><abstract>Personalization has been discussed in a number of domains and it also plays an important role in the area of human-computer interaction as users\' interaction abilities and preferences vary drastically. Considering these individual characteristics can contribute to better user experience and also accessibility of interactive settings. This extended abstract describes a framework that enables personalized interaction based on analyzing and modeling users\' interaction abilities.</abstract><author_list>Mirjam Augstein, Thomas Neumayr, Daniel Kern, Werner Kurschl, Josef Altmann, Thomas Burger</author_list></RECORD><RECORD><contentID>12563</contentID><title>An Interactive Points of Interest Guidance System</title><contentType>Poster</contentType><presentationID>10464</presentationID><abstract>In this paper we propose an intelligent user interface  for a Point-of-Interest (POI) recommendation system.  Our approach solves many challenges, such as learning from passive data, sequential real-time recommendations, Inferring the user\'s propensity to listen to a recommendation, and minimizing recommendation fatigue.  We demonstrate our approach on a real world POI data set from Flicker.</abstract><author_list>Georgios Theocharous, Nikos Vlassis, Zheng Wen</author_list></RECORD><RECORD><contentID>12565</contentID><title>DermaTrack: A Skin Cancer Tracking Intelligent Application</title><contentType>Poster</contentType><presentationID>10462</presentationID><abstract>Although, one in five humans living in high risk areas will develop skin cancer during a lifetime, there is currently no mechanism to help humans track the development of skin moles. DermaTrack, the application described in this paper offers an innovative mechanism, using a mobile application, for i) tracking skin cancer over time, and ii) share the data recorded with a specialized doctor. In this paper, we are providing an evaluation of the prototype mobile application interface developed.</abstract><author_list>Nikos Mouzouras, Andreas Pogiatzis, Styliani Kleanthous, George Samaras</author_list></RECORD><RECORD><contentID>12568</contentID><title>Evaluation of Attention Guiding Techniques for Augmented Reality-based Assistance in Picking and Assembly Tasks</title><contentType>Poster</contentType><presentationID>10465</presentationID><abstract>Intelligent personal assistance systems for manual tasks may support users on multiple levels. A general function is guiding the visual attention of the user towards the item relevant for the next action. This is a challenging task, as the user may be in arbitrary positions and orientations relative to the target. Optical see-through head-mounted-displays (HMDs) present an additional challenge, as the target may be already visible for the user but lie outside the field-of-view of the augmented reality (AR) display.&lt;/p&gt;In the context of a smart glasses-based assistance system for a manual assembly station, we evaluated five different visual attention guidance techniques for optical see-through devices. We found that combined directional and positional in-situ guidance performs best overall, but that performance depends on target location. The study is our first realization of a simulated AR methodology in which we create a repeatable and highly-controlled experimental design using a virtual reality (VR) HMD setup.</abstract><author_list>Patrick Renner, Thies Pfeiffer</author_list></RECORD><RECORD><contentID>12569</contentID><title>Exploring Emotions in Online Movie Reviews for Online Browsing</title><contentType>Poster</contentType><presentationID>10470</presentationID><abstract>A restaurant review is a reflection of the reviewer&#xC3;&#xA2;&#xE2;?&#xAC;&#xE2;?&#xA2;s experience and attitude towards the restaurant. The same applies to a review of a new phone or a review on any other online merchandize. Films, however, are created with the intended purpose to evoke an emotional response in the viewer. This emotional response does not necessarily correspond with the viewer&#xC3;&#xA2;&#xE2;?&#xAC;&#xE2;?&#xA2;s attitude towards the film. Thus, the question we try to address is, would the emotions expressed in a film&#xC3;&#xA2;&#xE2;?&#xAC;&#xE2;?&#xA2;s online reviews also reflect the emotions elicited during the film? In this work, we take a first step in the investigationof this question, by studying the role of emotions in movie reviews as expressed in a large dataset of millions of online reviews for over 9000 movies, that appeared in IMDb from 1972 to 2015. Our results show that we can extract emotions elicited by the film from its reviews, and create an emotional signature of a film, and of a genre. This is a first step towards an Emotion-based Film Browser UI system that will enable users to browse films according to the emotions they evoke.</abstract><author_list>Nadeem Bader, Osnat Mokryn, Joel Lanir</author_list></RECORD><RECORD><contentID>12570</contentID><title>EyamKayo: Interactive Gaze and Facial Expression Captcha</title><contentType>Poster</contentType><presentationID>10468</presentationID><abstract>This paper introduces {it EyamKayo}, a first-of-its-kind interactive CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart), using eye gaze and facial expression based human interactions, to better distinguish humans from software robots. Our system generates a sequence of instructions, asking the user to follow a controlled sequence of gaze points, and generate a controlled sequence of facial expressions. We evaluate user comfort and system usability, and validate using usability tests.</abstract><author_list>Utkarsh Dwivedi, Karan Ahuja, Rahul Islam, Ferdous A. Barbhuiya, Seema Nagar, Kuntal Dey</author_list></RECORD><RECORD><contentID>12573</contentID><title>Measuring Visual Search Ability on the Web</title><contentType>Poster</contentType><presentationID>10472</presentationID><abstract>Findability belongs to key aspects of a webpage usability. When testing findability, the measured task times result not only from the design of a web page, but are influenced also by the individual differences in the participants&#xC3;&#xA2;&#xE2;?&#xAC;&#xE2;?&#xA2; abilities, such as their visual search ability. In order to measure this ability, we designed a calibration procedure consisting of a visual search task containing Web icons. In this poster paper, we present results of a quantitative eye tracking study with 45 participants comparing the designed visual search task to the standard conjunction search with respect to the reaction time, number of fixations as well as the used search strategies. The results show that searching for icons is a harder task eliciting more fixations and longer reaction times. In addition, it allows us to differentiate the visual search ability of the users as indicated by the differences in reaction times and search strategies.</abstract><author_list>M&#xE1;ria Drag&#xFA;nov&#xE1;, Robert Moro, Maria Bielikova</author_list></RECORD><RECORD><contentID>12575</contentID><title>Monitoring Response Quality During Campimetry Via Eye-Tracking</title><contentType>Poster</contentType><presentationID>10475</presentationID><abstract>In a variety of use-cases, deriving information on user&#xC3;&#xA2;&#xE2;?&#xAC;&#xE2;?&#xA2;s fatigue is an important step for content adaptation. In this work, we investigate which eye tracking related measures can predict the error rate (as a proxy of subject&#xC3;&#xA2;&#xE2;?&#xAC;&#xE2;?&#xA2;s fatigue)during a visual experiment. Data was collected during a 40 minutes campimetric task, where the user has to detect visual stimuli (i.e., dots) of different contrast. We found that eye-tracking measures can be used to train a machine learning model to predict the error rate of a user with an average correlation of 0.72&#xC3;?&#xC2;&#xB1;0.17. The results show that this method can be used to measure the user&#xC3;&#xA2;&#xE2;?&#xAC;&#xE2;?&#xA2;s response quality.</abstract><author_list>Gustavo Vergani Dambros, Judith Ungewiss, Thomas K&#xFC;bler, Enkelejda Kasneci, Martin Sp&#xFC;ler</author_list></RECORD><RECORD><contentID>12577</contentID><title>Personalising Healthy Eating Messages to Age, Gender and Personality: Using Cialdini's Principles and Framing</title><contentType>Poster</contentType><presentationID>10459</presentationID><abstract>We examine how persuasive messages can be used to promote and encourage healthy eating based on personality. After a personality assessment, participants assessed the persuasiveness of messages designed using Cialdini's principles of persuasion. The results of our study indicate that 'Authority' messages were most influential. In addition, we observed that positively framed messages were significantly more persuasive than negatively framed ones. Furthermore, personality had a significant influence on the best message type, with agreeable subjects being more inclined to persuasion than others.</abstract><author_list>Rosemary Josekutty Thomas, Judith Masthoff, Nir Oren</author_list></RECORD><RECORD><contentID>12580</contentID><title>Rise of the Chatbots: Finding A Place for Artificial Intelligence in India and US</title><contentType>Poster</contentType><presentationID>10460</presentationID><abstract>This research study explores how chatbots can find a place in routine daily lives.  Chatbot development has increased while in many cases its purpose still remains loosely defined.  Due to its novel and relatively new technology, there is an opportunity to create meaningful experiences with chatbots in a typical personals life. Qualitative insights were collected from 54 participants in India and the US over the course of two weeks.  To identify opportunities for chatbots, we must understand how these programs are perceived and what needs exist for people.  The research objectives include understanding the following: 1) anticipations for chatbots 2) preferred input modalities 3) opportunities for chatbots based on user needs.</abstract><author_list>Jennifer Zamora</author_list></RECORD><RECORD><contentID>12579</contentID><title>SilverTouch: Game-based Training for Children with Myoelectric Prostheses</title><contentType>Poster</contentType><presentationID>10466</presentationID><abstract>In the context of Medicine, technology facilitates the design and development of prostheses that make it possible for patients to recover specific movements. Myoelectric prostheses connect to the patient nerves directly and allow limb movements via electric impulses generated by the nervous system. However, the use of these prostheses requires intensive training, which can be hard and tiring, especially for children. The use of games can make training much more enjoyable. In this paper, we describe SilverTouch, an application to help children to train the use of myoelectric prostheses by means of three different types of multi-touch games. The games are dynamically generated for each user according to his needs and performance at runtime. We have designed them following the advice of experts in Medicine, Physiotherapy, Therapy and Education. The results are promising: the final users agreed that SilverTouch is a good tool for training the use of prostheses, while the experts confirmed its potential to be widely utilized for that purpose.</abstract><author_list>Fernando G Costales, Rosa N Carro</author_list></RECORD><RECORD><contentID>12581</contentID><title>Summarizing Social Image Search Results using Human Affects</title><contentType>Poster</contentType><presentationID>10461</presentationID><abstract>In this paper, we propose the selection of representative images based on human affects. For this, the images are first transformed into the affective space using convolutional neural network (CNN). Thereafter, images are clustered on affective space and then the resulting clusters are ranked based on the proposed three properties &#xC3;&#xA2;&#xE2;?&#xAC;&#xE2;?? coverage, affective coherence and distinctiveness. Finally, some representative images are selected from top-ranked clusters. The experiments conducted on Flickr images showed the effectiveness of the proposed method.</abstract><author_list>Eunjeong Ko, Eun Kim, Yaohui Yu</author_list></RECORD><RECORD><contentID>12583</contentID><title>Towards Automatic Skill Evaluation in Microsurgery</title><contentType>Poster</contentType><presentationID>10467</presentationID><abstract>In the past decade, eye tracking has emerged as a promising answer to the increasing needs of understanding surgical expertise. The implicit desire is to design an intelligent user interface (IUI) to monitor and assess the competency of surgical trainees. In this paper, for the first time in microsurgery, we explore the potential for a surgical automatic skill assessment through a combination of machine learning techniques, computational modeling, and eye tracking. We present primary findings from a random forest classification method where we achieved about 70% recognition rate for the detection of expert and novice group. This leads us to a conclusion that prediction of the micro-surgeon performance is possible, can be automated, and that the eye movement data carry important information about the skills of micro-surgeons.</abstract><author_list>Shahram Eivazi, Michael Slupina, Wolfgang Fuhl, Hoorieh Afkari, Ahmad Hafez, Enkelejda Kasneci</author_list></RECORD><RECORD><contentID>12558</contentID><title>Towards Intelligent Surgical Microscope: Micro-surgeons\' Gaze and Instrument Tracking</title><contentType>Poster</contentType><presentationID>10463</presentationID><abstract>After many decades of research, the presence of intelligent user interfaces is unquestionable in any modern operating room (OR). For the first time, we aim to bring proactive intelligent systems into microsurgery OR. The first step towards an intelligent surgical microscope is to design an activity-aware microscope. In this paper, we present a novel system that we have built to record both eyes and instruments movements of surgeons while operating with a surgical microscope. We present a case study in micro-neurosurgery to show how the system monitors the surgeon\'s activities. We achieved about 1 mm accuracy for gaze and instrument tracking. Now real-time ecologically valid data can be used to design, for example, a self-adjustable microscope.</abstract><author_list>Shahram Eivazi, Wolfgang Fuhl, Enkelejda Kasneci</author_list></RECORD><RECORD><contentID>12585</contentID><title>Using Virtual Reality to Train Designers to Develop Friendly Interfaces for Achromatic Vision Patients</title><contentType>Poster</contentType><presentationID>10474</presentationID><abstract>An investigation in the use of Virtual Reality as a means of training designers to design interfaces accessible to achromatic vision patients is presented. Within this context virtual environments incorporating real life environments are visualised through the eyes of achromatic vision patients and designers are given the opportunity to navigate and interact with the virtual environment using different types of interaction schemes. Through the process designers assess the applicability of different interaction methods adjusted to the needs of achromatic vision patients. According to the results of an experimental investigation, the idea of using Virtual Reality-based training is deemed effective.</abstract><author_list>Vaso Constantinou, Andreas Lanitis, Andri Ioannou</author_list></RECORD><RECORD><contentID>12587</contentID><title>Visualizing Insider Threats:  An Effective Interface for Security Analytics</title><contentType>Poster</contentType><presentationID>10473</presentationID><abstract>With the ever-growing volume of cyber-attacks on organizations, security analysts require effective visual interfaces and interaction techniques to detect security breaches and, equally importantly, to efficiently share threat information. To support this need, we present a tool called &#xC3;&#xA2;&#xE2;?&#xAC;&#xC5;?User Behavior Analytics&#xC3;&#xA2;&#xE2;?&#xAC;&#xC2;&#x9D; (UBA) that conducts continuous analysis of individuals\' usage of their organizational IT networks, and effectively visualizes the associated security exposures of the organization. The UBA tool was developed as an extension of IBM&#xC3;&#xA2;&#xE2;?&#xAC;&#xE2;?&#xA2;s security analytics environment, and incorporates a risk-focused dashboard that highlights anomalous user behaviors and the aggregated risk levels associated with individual users, user groups, and overall system security state. Moreover, the tool&#xC3;&#xA2;&#xE2;?&#xAC;&#xE2;?&#xA2;s dashboard has been designed to facilitate rapid review of security incidents and correlate them with data from various sources such as user directory and HR systems. In doing so, the tool presents busy security analysts with an effective means to visually identify and respond to cyber threats on the organization\'s crown jewels.</abstract><author_list>Bar Haim, Y Wolfsthal, Eitan Menahem, Christopher Meenan</author_list></RECORD><RECORD><contentID>12588</contentID><title>Visualizing Spatial-Temporal Evaluation of News Stories</title><contentType>Poster</contentType><presentationID>10471</presentationID><abstract>News today are generated and distributed online by a multitude of sources all over the world. Easy and efficient monitoring and analysis of news stories is of interest to both professional analysts and the general public. One interesting aspect is the magnitude and impact of a story as well as its evolution over time. In this work we introduce an idea and a system that presents temporal and spatial evolution of news world-wide, in two different levels, to help users quickly understand and act upon the large amount of data. An overview option shows a general split of the reported news, and a more detailed view provides interactive options for deeper analysis of a single news episode. We demonstrate our system on data from news events generated by the Europe Media Monitor (EMM), an online news aggregator platform.</abstract><author_list>Julia Sheidin, Joel Lanir, Tsvi Kuflik, Peter Bak</author_list></RECORD><RECORD><contentID>12589</contentID><title>VisualMath: An Automated Visualization System for Understanding Math Word-Problems</title><contentType>Poster</contentType><presentationID>10469</presentationID><abstract>Math word problems are difficult for students to start with since they involve understanding the problem&#xC3;&#xA2;&#xE2;?&#xAC;&#xE2;?&#xA2;s context and abstracting out its underlying mathematical operations. A visual understanding of the problem at hand can be very useful for the comprehension of the problem. We present a system VisualMath that uses machine learning tools and crafted visual logic to automatically generate appropriate visualizations from the text of the word-problems and solve it. We demonstrate the improvements in the understanding of math word-problems by conducting a user study and learning of meaning of relevant new words by students.</abstract><author_list>Utkarsh Dwivedi, Nitendra Rajput, Prasenjit Dey, Blessin Varkey</author_list></RECORD><RECORD><contentID>12536</contentID><title>An Intelligent Interface for Organizing Online Opinions on Controversial Topics</title><contentType>Short Paper</contentType><presentationID>10449</presentationID><abstract>An enormous amount of posts and comments are shared in online social forums, which often organize these online social opinions based on semantic contents. However, for controversial topics, people with different attitudes and stances often have very distinct perspectives, reactions, and emotions to the same post. Organization by semantic contents often encourages selective exposure to information,  which may exacerbate opinion polarization. To address this problem, we design a novel interface that allows people to better understand and appreciate people with different stances in social forums. Our interface was developed to allow interactive visualization and categorization of original posts about a controversial topic with crowd workers\' reactions and emotions from different stances. We evaluated the interface using Reddit posts about US presidential candidates. Results demonstrate that the interface can mitigate selective exposure and help users to adopt a broader spectrum of opinions than the traditional Reddit interface. </abstract><author_list>Mingkun Gao, Hyo Jin Do, Wai-Tat Fu</author_list></RECORD><RECORD><contentID>12524</contentID><title>Assessing the Contribution of Twitter&#xB4;s Textual Information to Graph-based Recommendation</title><contentType>Short Paper</contentType><presentationID>10510</presentationID><abstract>Graph-based recommendation approaches can model associations between users  and items alongside additional contextual information. Recent studies  demonstrated that representing features extracted from social media  (SM) auxiliary data, like friendships, jointly with traditional  users/items ratings in the graph, contribute to recommendation  accuracy. In this work, we take a step further and propose an extended  graph representation that includes socio-demographic and personal  traits extracted from the content posted by the user on SM. Empirical  results demonstrate that processing unstructured textual information  collected from Twitter and representing it in structured form in the  graph improves recommendation performance, especially in cold start  conditions.</abstract><author_list>Evgenia Wasserman Pritsker, Tsvi Kuflik, Einat Minkov</author_list></RECORD><RECORD><contentID>12521</contentID><title>Criteria Chains: A Novel Multi-Criteria Recommendation Approach</title><contentType>Short Paper</contentType><presentationID>10439</presentationID><abstract>Recommender systems (RSs) have been successfully applied to alleviate the problem of information overload and assist users\' decision makings. Multi-criteria recommender systems is one of the RSs which utilizes users\' multiple ratings on different aspects of the items (i.e., multi-criteria ratings) to predict user preferences. Traditional approaches usually predict ratings on each criterion individually and aggregate them together to estimate the user preferences. In this paper, we propose an approach named as \</abstract><author_list>Yong Zheng</author_list></RECORD><RECORD><contentID>12513</contentID><title>Deep Sequential Recommendation for Personalized Adaptive User Interfaces</title><contentType>Short Paper</contentType><presentationID>10523</presentationID><abstract>Adaptive user-interfaces (AUIs) can enhance the usability of complex software by providing real-time contextual adaptation and assistance. Ideally, AUIs should be personalized and versatile, i.e., able to  adapt to each user who may perform a variety of complex tasks. But this is difficult to achieve with  many interaction elements when data-per-user is sparse. In this paper, we propose an architecture for personalized AUIs that leverages upon developments in (1) deep learning, particularly gated recurrent units, to efficiently learn user interaction patterns, (2) collaborative filtering techniques that enable sharing of data among users, and (3) fast approximate nearest-neighbor methods in Euclidean spaces for quick UI control and/or content recommendations. Specifically, interaction histories are embedded in a learned space along with users and interaction elements; this allows the AUI to query and recommend likely next actions based on similar usage patterns across the user base. In a comparative evaluation on user-interface, web-browsing and e-learning datasets, the deep recurrent neural-network (DRNN) outperforms state-of-the-art tensor-factorization and metric embedding methods.</abstract><author_list>Harold Soh, Scott Sanner, Madeleine White, Greg Jamieson</author_list></RECORD><RECORD><contentID>12526</contentID><title>Eyes Understand the Sketch!: Gaze-Aided Stroke Grouping of Hand-Drawn Flowcharts</title><contentType>Short Paper</contentType><presentationID>10444</presentationID><abstract>Stroke grouping in sketch recognition is both difficult and time-consuming. Our preliminary experiment indicates that, when people drawing flowcharts, their gaze focused on non-arrow areas, which providing a spatial cue for stroke grouping. Therefore, we present a novel stroke grouping method aided by gaze information. Based on gaze data that is collected simultaneously during natural drawing process, we generate hotspot areas serving as the position reference of semantic symbols. Strokes are first roughly grouped by the hotspot areas, so as to efficiently decrease the searching space. Experiment on a dataset of 54 flowcharts shows that time efficiency of stroke grouping can be greatly improved in our method and there is much potential for introducing eye-gaze data in sketch recognition.</abstract><author_list>Beibei Chao, Xiaoyan Zhao, Dapeng Shi, Guihuan Feng, Bin Luo</author_list></RECORD><RECORD><contentID>12489</contentID><title>GPGPU-based Highly Parallelized 3D Node Localization for Real-Time 3D Model Reproduction</title><contentType>Short Paper</contentType><presentationID>10454</presentationID><abstract>This paper proposes a highly parallelized 3D node localization method based on cross-entropy method for the 3D modeling system. Cross-entropy localization statistically estimates node positions from node-to-node distance information by sampling, and each sample evaluation and internal computation of objective function can be processed in parallel. Experimental results show our GPGPU-based implementation achieved 5,163x and 61.5x speed up compared to a single processor and 80-processor implementations. In addition, for enhancing model reproduction accuracy, this work introduces a penalty function to mitigate flip ambiguity.</abstract><author_list>Kauzki Hirosue, Shohei Ukawa, Yuichi Itoh, Takao Onoye, Masanori Hashimoto</author_list></RECORD><RECORD><contentID>12506</contentID><title>GUI Design for IDE Command Recommendations</title><contentType>Short Paper</contentType><presentationID>10524</presentationID><abstract>This paper describes a novel design of a graphical user interface (GUI) to recommend useful command within an integrated development environment. The recommendation GUI contains a description of the suggested command, an explanation why the command is recommended, and a command usage example. The proposed design is based on the analysis of relevant guidelines identified in the literature. Its perceived usability and acceptance were evaluated in a live user study with 36 software developers. Our findings, partially contradicting existing literature, indicate that the presentation of the command-the description and the example-is perceived as more useful than the explanation of the rationale for the recommendation.</abstract><author_list>Marko Gasparic, Andrea Janes, Francesco Ricci, Marco Zanellati</author_list></RECORD><RECORD><contentID>12494</contentID><title>Guidelines for Tree-based Collaborative Goal Setting</title><contentType>Short Paper</contentType><presentationID>10499</presentationID><abstract>Educational technology needs a model of learning goals to support motivation, learning gain, tailoring of the learning process, and sharing of the personal goals between different types of users (i.e., learner and educator) and the system. This paper proposes a tree-based learning goal structuring to facilitate personal goal setting to shape and monitor the learning process. We developed a goal ontology and created a user interface representing this knowledge-base for the self-management education for children with Type 1 Diabetes Mellitus. Subsequently, a co-operative evaluation was conducted with healthcare professionals to refine and validate the ontology and its representation. Presentation of a concrete prototype proved to support professionals\' contribution to the design process. The resulting tree-based goal structure enables three important tasks: ability assessment, goal setting and progress monitoring. Visualization should be clarified by icon placement and clustering of goals with the same difficulty and topic. Bloom\'s taxonomy for learning objectives should be applied to improve completeness and clarity of goal content.</abstract><author_list>Rifca Peters, Joost Broekens, Mark Neerincx</author_list></RECORD><RECORD><contentID>12537</contentID><title>I-SED: an Interactive Sound Event Detector</title><contentType>Short Paper</contentType><presentationID>10515</presentationID><abstract>Tagging of sound events is essential in many research areas. However, finding sound events and labeling them within a long audio file is tedious and time-consuming. Building an automatic recognition system using machine learning techniques is often not feasible because it requires a large number of human-labeled training examples and fine tuning the model for a specific application. Fully automated labeling is also not reliable enough for all uses. We present I-SED, an interactive sound detection interface using a human-in-the-loop approach that lets a user reduce the time required to label audio that is tediously long (e.g. 20 hours) to do manually and has too few prior labeled examples (e.g. one) to train a state-of-the-art machine audio labeling system. We performed a human-subject study to validate its effectiveness and the results showed that our tool helped participants label all target sound events within a recording twice as fast as labeling them manually.</abstract><author_list>Bongjun Kim, Bryan Pardo</author_list></RECORD><RECORD><contentID>12532</contentID><title>Inline Co-Evolution between Users and Information Presentation for Data Exploration</title><contentType>Short Paper</contentType><presentationID>10479</presentationID><abstract>This paper presents an intelligent user interface model dedicated to the exploration of complex databases. This model is implemented on a 3D metaphor: a virtual museum. In this metaphor, the database elements are embodied as museum objects. The objects are grouped in rooms according to their semantic properties and relationships and the rooms organization forms the museum. Rooms&#xB4; organization is not predefined but defined incrementally by taking into account not only the relationships between objects, but also the user&#xB4;s centers of interest. The latter are evaluated in real-time through user interactions within the virtual museum. This interface allows for a personal reading and favors the discovery of unsuspected links between data. In this paper, we present our model\'s formalization as well as its application to the context of cultural heritage.</abstract><author_list>Landy Rajaonarivo, Matthieu Courgeon, Eric Maisel, Pierre De Loor</author_list></RECORD><RECORD><contentID>12487</contentID><title>Interactive Elicitation of Knowledge on Feature Relevance Improves Predictions in Small Data Sets</title><contentType>Short Paper</contentType><presentationID>10514</presentationID><abstract>Providing accurate predictions is challenging for machine learning algorithms when the number of features is larger than the number of samples in the data. Prior knowledge can improve machine learning models by indicating relevant variables and parameter values. Yet, this prior knowledge is often tacit and only available from domain experts. We present a novel approach that uses interactive visualization to elicit the tacit prior knowledge and uses it to improve the accuracy of prediction models. The main component of our approach is a user model that models the domain expert\'s knowledge of the relevance of different features for a prediction task. In particular, based on the expert\'s earlier input, the user model guides the selection of the features on which to elicit user\'s knowledge next. The results of a controlled user study show that the user model significantly improves prior knowledge elicitation and prediction accuracy, when predicting the relative citation counts of scientific documents in a specific domain.</abstract><author_list>Luana Micallef, Iiris Sundin, Pekka Marttinen, Muhammad Ammad-ud-din, Tomi Peltola, Marta Soare, Giulio Jacucci, Samuel Kaski</author_list></RECORD><RECORD><contentID>12538</contentID><title>Learning to Rate Clinical Concepts Using Simulated Clinician Feedback</title><contentType>Short Paper</contentType><presentationID>10509</presentationID><abstract>We present a user-based model for rating concepts (i.e., words and phrases) in clinical queries based on their relevance to clinical decision making. Our approach can be adopted by information retrieval systems (e.g., search engines) to identify the most important concepts in user queries in order to better understand user intent and to improve search results. In our experiments, we examine several learning algorithms and show that by using simulated user feedback, our approach can predict the ratings of the clinical concepts in newly unseen queries with high prediction accuracy.   </abstract><author_list>Mohammad R Alsulmi, Benjamin A Carterette</author_list></RECORD><RECORD><contentID>12500</contentID><title>LyriSys: An Interactive Support System for Writing Lyrics Based on Topic Transition</title><contentType>Short Paper</contentType><presentationID>10516</presentationID><abstract>This paper presents LyriSys, a novel lyric-writing support system. Previous systems for lyric writing can fully automatically only generate a single line of lyrics that satisfies given constraints on accent and syllable patterns or an entire lyric. In contrast to such systems, LyriSys allows users to create and revise their work incrementally in a trial-and-error manner. Through fine-grained interactions with the system, the user can create the specifications of the musical structure and the story of the lyrics in terms of the verse-bridge-chorus structure, the number of lines, words and syllables, and most importantly, the transition over semantic topics such as &#xE2;??&#xC3;&#xBC;&#xC2;&#xAE;scene&#xE2;??&#xC3;&#xBC;&#xC2;&#xA9;, &#xE2;??&#xC3;&#xBC;&#xC2;&#xAE;dark&#xE2;??&#xC3;&#xBC;&#xC2;&#xA9; and &#xE2;??&#xC3;&#xBC;&#xC2;&#xAE;sweet love&#xE2;??&#xC3;&#xBC;&#xC2;&#xA9;. This paper provides an overview of the design of the system and its user interface and describes how the writing process is guided by a state-of-the-art probabilistic generative topic model that is trained without supervision. The system works for both Japanese and English.</abstract><author_list>Kento Watanabe, Yuichiroh Matsubayashi, Kentaro Inui, Tomoyasu Nakano, Satoru Fukayama, Masataka Goto</author_list></RECORD><RECORD><contentID>12527</contentID><title>Multi-faceted Index driven Navigation for Educational Videos in Mobile Phones</title><contentType>Short Paper</contentType><presentationID>10494</presentationID><abstract>One of the challenges that is holding back wide spread consumption of educational videos on mobile devices is the lack of mobile interfaces which can provide efficient video navigation capabilities. In this paper, we utilize multi-modal data analysis techniques which include analysis of the spoken content and the written content of the video, to create a multi-faceted index. We present a novel and   first-of-its-kind mobile interface which uses aforementioned multi-faceted index to provide intuitive, usable, and efficient way to navigate through a video. The efficacy of the proposed multi-faceted index driven mobile interface for non-linear navigation is demonstrated through a preliminary user study of $15$  participants. We demonstrate that the proposed interface leads to statistically significant savings in navigation time as compared to that of a baseline interface used by leading e-learning providers.</abstract><author_list>Abhishek Kumar, Kushal Srivastava, Kuldeep Yadav, Om Deshmukh</author_list></RECORD><RECORD><contentID>12484</contentID><title>QuickReview: A Novel Data-Driven Mobile User Interface for Reporting Problematic App Features</title><contentType>Short Paper</contentType><presentationID>10511</presentationID><abstract>User-reviews of mobile applications provide information that benefits other users and developers. Even though reviews contain feedback about an app&#xB4;s performance and problematic features, users and app developers need to spend considerable effort reading and analyzing the feedback provided. In this work, we introduce and evaluate QuickReview, an intelligent user interface for reporting problematic app features. Preliminary user evaluations show that QuickReview facilitates users to add reviews swiftly with ease, and also helps developers with quick interpretation of submitted reviews by presenting a ranked list of commonly reported features.</abstract><author_list>Tavita Su'a, Sherlock A. Licorish, Bastin Tony Roy Savarimuthu, Tobias Langlotz</author_list></RECORD><RECORD><contentID>12503</contentID><title>Session-Based Recommendations Using Item Embedding</title><contentType>Short Paper</contentType><presentationID>10520</presentationID><abstract>Recent methods for learning vector space representations of words, word embedding, such as GloVe and Word2Vec have succeeded in capturing fine-grained semantic and syntactic regularities. We analyzed the effectiveness of these methods for e-commerce recommender systems by transferring the sequence of items generated by users&#xB4; browsing journey in an e-commerce website into a sentence of words.     We examined the prediction of fine-grained item similarity (such as item most similar to iPhone 6 64GB smart phone) and item analogy (such as iPhone 5 is to iPhone 6 as Samsung S5 is to Samsung S6) using real life users&#xB4; browsing history of an online European department store. Our results reveal that such methods outperform related models such as singular value decomposition (SVD) with respect to item similarity and analogy tasks across different product categories.     Furthermore, these methods produce a highly condensed item vector space representation, item embedding, with behavioral meaning sub-structure. These vectors can be used as features in a variety of recommender system applications. In particular, we used these vectors as features in a neural network based models for anonymous user recommendation based on session&#xB4;s first few clicks. It is found that recurrent neural network that preserves the order of user&#xB4;s clicks outperforms standard neural network, item-to-item similarity and SVD (recall@10 value of 42% based on first three clicks) for this task.</abstract><author_list>Asnat Messica, Lior Rokach, Michael Friedman</author_list></RECORD><RECORD><contentID>12498</contentID><title>The Influence of Personality Traits and Cognitive Load on the Use of Adaptive User Interfaces</title><contentType>Short Paper</contentType><presentationID>10489</presentationID><abstract>One of the problems adaptive interfaces must solve is the issue of stability---users must be able to complete a familiar task reliably.  Split Adaptive Interfaces, where a limited part of the screen contains copies of the interface elements predicted to be of immediate use, are one technique for resolving this difficulty.  While prior work demonstrated that Split Adaptive Interfaces improve performance on average, the results of our study demonstrate systematic individual differences in the utilization of the adaptive features, which correlate with the stable user traits of Need for Cognition and Extraversion.  Specifically, higher Need for Cognition (a willingness to undertake difficult mental activities) is correlated with increased utilization rates, while higher Extraversion (a general orientation towards seeking gratification from the external world) is negatively correlated with utilization rates.  Our results also demonstrate a significant negative correlation between cognitive load induced by a secondary task and the utilization of the adaptive features.  This effect, however, is very small (less than two percentage points). Together, these results provide additional evidence of the usefulness of the split adaptive interface approach and a negligible effect of additional cognitive load, but also demonstrate that the approach does not benefit all users equally.</abstract><author_list>Krzysztof Gajos, Krysta Chauncey</author_list></RECORD><RECORD><contentID>12511</contentID><title>Towards Fine-Grained Adaptation of Exploration/Exploitation in Information Retrieval</title><contentType>Short Paper</contentType><presentationID>10519</presentationID><abstract>Lookup and exploratory search tasks can be distinguished using individuals&#xB4; information search behaviour. Previous work, however, has treated these search tasks as belonging to homogeneous categories, ignoring the specific information needs between users and even between search sessions for the same user. In this work, we avoid this dichotomy by considering each search task to exist on a spectrum between lookup and exploratory. In doing so, our approach aims to dynamically adapt exploration and exploitation in a manner commensurate with the user&#xB4;s individual requirements for each search session. We present a novel study design together with a regression model for predicting the optimal exploration rate based on simple metrics from the first iteration, such as clicks and reading time, that can be collected without special hardware. We perform model selection based on the data collected from a user study and show that predictions are consistent with user feedback.</abstract><author_list>Alan Medlar, Joel Pyykk&#xF6;, Dorota Glowacka</author_list></RECORD><RECORD><contentID>12509</contentID><title>Towards Understanding Human Mistakes of Programming by Example:  An Online User Study</title><contentType>Short Paper</contentType><presentationID>10484</presentationID><abstract>Programming-by-Example (PBE) enables users to create programs without writing a line of code. However, there is little research on people\'s ability to accomplish complex tasks by providing examples, which is the key to successful PBE solutions. This paper presents an online user study, which reports observations on how well people decompose complex tasks, and disambiguate sub-tasks. Our findings suggest that disambiguation and decomposition are difficult for inexperienced users. We identify seven types of mistakes made, and suggest new opportunities for actionable feedback based on unsuccessful examples, with design implications for future PBE systems.    </abstract><author_list>Tak Yeon Lee, Casey Dugan, Benjamin Bederson</author_list></RECORD><RECORD><contentID>12523</contentID><title>Use of Haptic Feedback to Train Correct Application of Force in Endodontic Surgery</title><contentType>Short Paper</contentType><presentationID>10504</presentationID><abstract>With the minute margins of error in endodontic surgery, training in manual dexterity and proper instrument handling are crucial components in the dental curriculum.  Important parameters include tool path, tool angulation, and force applied.  In this work, we focus on training of correct application of force.  This is particularly challenging since the amounts of force used are on the order of tenths of Newtons, requiring a highly refined tactile sense and incorrect force can cause irreversible damage.  Too great a force can cause overdrilling or in extreme cases perforation of the tooth.  Too small a force can cause thermal irritation possibly resulting in tissue necrosis.  Despite the importance of correct use of force, this is the dimension on which students receive the least tutorial feedback since force information is typically not available in traditional training settings. In this paper, we present an approach to using haptic feedback as a means to convey formative feedback on the correct application of force. Feedback is conveyed to the student graphically and the correct amount of force to apply is trained haptically.  The simulator is rewound and the student is asked to redo the stage where the error occurred.  Preliminary evaluation against a control group of students who received only feedback concerning outcome shows the feedback mechanism to be effective.</abstract><author_list>Myat Su Yin, Peter Haddawy, Siriwan Suebnukarn, Holger Schultheis, Phattanapon Rhienmora</author_list></RECORD><RECORD><contentID>12592</contentID><title>A Multimodal Approach to Assessing User Experiences with Agent Helpers</title><contentType>TiiS Paper</contentType><presentationID>10446</presentationID><abstract>The study of agent helpers using linguistic strategies such as vague language and politeness has often come across obstacles. One of these is the quality of the agent's voice and its lack of appropriate fit for using these strategies. The first approach of this article compares human vs. synthesised voices in agents using vague language. This approach analyses the 60,000-word text corpus of participant interviews to investigate the differences of user attitudes towards the agents, their voices and their use of vague language. It discovers that while the acceptance of vague language is still met with resistance in agent instructors, using a human voice yields more positive results than the synthesised alternatives. The second approach in this article discusses the development of a novel multimodal corpus of video and text data to create multiple analyses of human-agent interaction in agent-instructed assembly tasks. The second approach analyses user spontaneous facial actions and gestures during their interaction in the tasks. It found that agents are able to elicit these facial actions and gestures and posits that further analysis of this nonverbal feedback may help to create a more adaptive agent. Finally, the approaches used in this article suggest these can contribute to furthering the understanding of what it means to interact with software agents.</abstract><author_list>Leigh Clark, Abdulmalik Ofemile, Svenja Adolphs, Tom Rodden</author_list></RECORD><RECORD><contentID>12594</contentID><title>Agents Vs. Users: Visual Recommendation of Research Talks with Multiple Dimension of Relevance</title><contentType>Tiis Paper</contentType><presentationID>10481</presentationID><abstract>Several approaches have been researched to help people deal with abundance of information. An important feature pioneered by social tagging systems and later used in other kinds of social systems is the ability to explore different community relevance prospects by examining items bookmarked by a specific user or items associated by various users with a specific tag. A ranked list of recommended items offered by a specific recommender engine can be considered as another relevance prospect. The problem that we address is that existing personalized social systems do not allow their users to explore and combine multiple relevance prospects. Only one prospect can be explored at any given time&#x2014;a list of recommended items, a list of items bookmarked by a specific user, or a list of items marked with a specific tag. In this article, we explore the notion of combining multiple relevance prospects as a way to increase effectiveness and trust. We used a visual approach to recommend articles at a conference by explicitly presenting multiple dimensions of relevance. Suggestions offered by different recommendation techniques were embodied as recommender agents to put them on the same ground as users and tags. The results of two user studies performed at academic conferences allowed us to obtain interesting insights to enhance user interfaces of personalized social systems. More specifically, effectiveness and probability of item selection increase when users are able to explore and interrelate prospects of items relevance&#x2014;that is, items bookmarked by users, recommendations and tags. Nevertheless, a less-technical audience may require guidance to understand the rationale of such intersections.</abstract><author_list>Katrien Verbert, Denis Parra, Peter Brusilovsky</author_list></RECORD><RECORD><contentID>12595</contentID><title>Effects of the Advisor and Environment on Requesting and Complying With Automated Advice</title><contentType>Tiis Paper</contentType><presentationID>10486</presentationID><abstract>Given the rapid technological advances in our society and the increase in artificial and automated advisors with whom we interact on a daily basis, it is becoming increasingly necessary to understand how users interact with and why they choose to request and follow advice from these types of advisors. More specifically, it is necessary to understand errors in advice utilization. In the present study, we propose a methodological framework for studying interactions between users and automated or other artificial advisors. Specifically, we propose the use of virtual environments and the tarp technique for stimulus sampling, ensuring sufficient sampling of important extreme values and the stimulus space between those extremes. We use this proposed framework to identify the impact of several factors on when and how advice is used. Additionally, because these interactions take place in different environments, we explore the impact of where the interaction takes place on the decision to interact. We varied the cost of advice, the reliability of the advisor, and the predictability of the environment to better understand the impact of these factors on the overutilization of suboptimal advisors and underutilization of optimal advisors. We found that less predictable environments, more reliable advisors, and lower costs for advice led to overutilization, whereas more predictable environments and less reliable advisors led to underutilization. Moreover, once advice was received, users took longer to make a final decision, suggesting less confidence and trust in the advisor when the reliability of the advisor was lower, the environment was less predictable, and the advice was not consistent with the environmental cues. These results contribute to a more complete understanding of advice utilization and trust in advisors.</abstract><author_list>Steven C. Sutherland, Casper Harteveld, Michael E. Young</author_list></RECORD><RECORD><contentID>12596</contentID><title>Providing Arguments in Discussions on the Basis of the Prediction of Human Argumentative Behavior</title><contentType>Tiis Paper</contentType><presentationID>10496</presentationID><abstract>Argumentative discussion is a highly demanding task. In order to help people in such discussions, this article provides an innovative methodology for developing agents that can support people in argumentative discussions by proposing possible arguments. By gathering and analyzing human argumentative behavior from more than 1000 human study participants, we show that the prediction of human argumentative behavior using Machine Learning (ML) is possible and useful in designing argument provision agents. This paper first demonstrates that ML techniques can achieve up to 76% accuracy when predicting people&#x2019;s top three argument choices given a partial discussion. We further show that well-established Argumentation Theory is not a good predictor of people&#x2019;s choice of arguments. Then, we present 9 argument provision agents, which we empirically evaluate using hundreds of human study participants. We show that the Predictive and Relevance-Based Heuristic agent (PRH), which uses ML prediction with a heuristic that estimates the relevance of possible arguments to the current state of the discussion, results in significantly higher levels of satisfaction among study participants compared with the other evaluated agents. These other agents propose arguments based on Argumentation Theory; propose predicted arguments without the heuristics or with only the heuristics; or use Transfer Learning methods. Our findings also show that people use the PRH agents proposed arguments significantly more often than those proposed by the other agents.</abstract><author_list>Ariel Rosenfeld, Sarit Kraus</author_list></RECORD><RECORD><contentID>12593</contentID><title>VizRec: Recommending Personalized Visualizations</title><contentType>Tiis Paper</contentType><presentationID>10456</presentationID><abstract>Visualizations have a distinctive advantage when dealing with the information overload problem: Because they are grounded in basic visual cognition, many people understand them. However, creating proper visualizations requires specific expertise of the domain and underlying data. Our quest in this article is to study methods to suggest appropriate visualizations autonomously. To be appropriate, a visualization has to follow known guidelines to find and distinguish patterns visually and encode data therein. A visualization tells a story of the underlying data; yet, to be appropriate, it has to clearly represent those aspects of the data the viewer is interested in. Which aspects of a visualization are important to the viewer? Can we capture and use those aspects to recommend visualizations? This article investigates strategies to recommend visualizations considering different aspects of user preferences. A multi-dimensional scale is used to estimate aspects of quality for visualizations for collaborative filtering. Alternatively, tag vectors describing visualizations are used to recommend potentially interesting visualizations based on content. Finally, a hybrid approach combines information on what a visualization is about (tags) and how good it is (ratings). We present the design principles behind VizRec, our visual recommender. We describe its architecture, the data acquisition approach with a crowd sourced study, and the analysis of strategies for visualization recommendation.</abstract><author_list>Belgin Mutlu, Eduardo Veas, Christoph Trattner</author_list></RECORD><RECORD><contentID>12683</contentID><title>From IoT to WoT: Can We Reproduce the Success of HTML for Smart Environments</title><contentType>Workshop Keynote</contentType><presentationID>10742</presentationID><abstract>Interaction within pervasive environments is still, predominantly, characterized by isolatedapplications constrained to the ecosystem of a consortium or even only a single vendor. Whenwe regard the success of standardized markup languages, such as HTML and related technologies,to transform the Internet onto the World Wide Web, the question arises whether a similartransformation for the interaction with smart objects in a pervasive environment can beduplicated. While first attempts at reference architectures and declarative descriptions forinteraction in general predate even the Internet, their applicability for interfaces in thescope of smart objects in a pervasive environment gained new momentum with recent W3Crecommendations of their working group for multimodal interaction and subsequent ambitions todevelop standards for the emph{Web of Things}. If we are to ever commoditize smart objects and realize the vision of Mark Weiser, common conceptualizations, interoperable implementations and standardized interfaces with a sufficient momentum to gain industrial support will need to be developed.In this talk I will give an introduction about the development of reference models formultimodal interaction, the problems and techniques of dialog management and develop anoverview of the current ambitions to standardize interfaces and interactions for smartenvironments.</abstract><author_list>Stefan Radomski</author_list></RECORD><RECORD><contentID>12635</contentID><title>From Search to Discovery with Visual Exploration Tools</title><contentType>Workshop Keynote</contentType><presentationID>10716</presentationID><abstract>In our goal to personalize the discovery of scientific information, we built systems using visual analytics principles for exploration of textual documents [1]. The concept was extended to explore information quality of user generated content [2]. Our interfaces build upon a cognitive model, where awareness is a key step of exploration [3].In education-related circles, a frequent concern is that people increasingly need to know how to search, and that knowing how to search leads to finding information efficiently. The ever-growing information overabundance right at our fingertips needs a natural skill to develop and refine search queries to get better search results, or does it?Exploratory search is an investigative behavior we adopt to build knowledge by iteratively selecting interesting features that lead to associations between representative items in the information space [4,5]. Formulating queries was proven more complicated for humans than recognizing information visually [6]. Visual analytics takes the form of an open ended dialog between the user and the underlying analytics algorithms operating on the data [7]. This talk describes studies on exploration and discovery with visual analytics interfaces that emphasize transparency and control features to trigger awareness. We will discuss the interface design and the studies of visual exploration behavior.</abstract><author_list>Eduardo E. Veas</author_list></RECORD><RECORD><contentID>12676</contentID><title>Intelligent Interfaces for Open Social Student Modeling</title><contentType>Workshop Keynote</contentType><presentationID>10755</presentationID><abstract>In this talk I will introduce the emerging technology of Open Social Student Modeling (OSSM) and review several projects performed in our research lab to investigate the potential of OSSM. OSSM is a recent extension of Open Student Modeling (OSM), a popular technology in the area of personalized learning systems. While in traditional personalized systems, student models were hidden &#xE2;??under the hood&#xE2;?&#x9D; and used to personalize the educational process; open student modeling introduced the ability to view and modify the state of students&#xE2;?? own knowledge to support reflection, self-organized learning, and system transparency. Open Social Student Modeling takes this idea one step further by allowing students to explore each other&#xE2;??s models or an aggregated model of the class.  The idea to make OSM social was originally suggested and explored by Bull [1; 2]. Over the last few years, our team explored several approaches to present OSSM in a highly visual form and evaluated these approaches in a sequence of classroom and lab studies. I will present a summary of this work introducing such systems as QuizMap [3], Progressor [4], and Mastery Grids [5] and reviewing most interesting research evidence collected by the studies.</abstract><author_list>Peter Brusilovsky</author_list></RECORD><RECORD><contentID>12634</contentID><title>Personalization in the Context of Relevance-Based Visualization</title><contentType>Workshop Keynote</contentType><presentationID>10717</presentationID><abstract>In this talk, I will review our research attempts to implement different kinds of personalization in the context of relevance-based visualization. The goal of this research stream is to make relevance-based visualization adaptive to user long-term goals, interests, or prospects rather just responsive to short term immediate needs such as query terms. I will present four personalized relevance-based visualization systems: Adaptive VIBE, TalkExplorer, SetFusion, and IntersectionExplorer, For each system, I will present its idea, some evaluation results, and lessons learned.</abstract><author_list>Peter Brusilovsky</author_list></RECORD><RECORD><contentID>12714</contentID><title>User Aware Interaction Design for Recommender Systems</title><contentType>Workshop Keynote</contentType><presentationID>10708</presentationID><abstract>User Aware Interaction Design for Recommender Systems</abstract><author_list>John O'Donovan</author_list></RECORD><RECORD><contentID>12612</contentID><title>A Feasible BCI in Real Life: Using Predicted Head Rotation to Improve HMD Imaging</title><contentType>Workshop Paper</contentType><presentationID>10735</presentationID><abstract>While brain signals potentially provide us with valuable information about a user, it is not straightforward to derive and use this information to smooth man-machine interaction in a real life setting. We here propose to predict head rotation on the basis of brain signals in order to improve images presented in a Head Mounted Display (HMD). Previous studies based on arm and leg movements suggest that this could be possible, and a pilot study showed promising results. From the perspective of the field of Brain-Computer Interfaces (BCI), this application provides a good case to put the field&#xE2;??s achievements to the test and to further develop in the context of a real life application. The main reason for this is that within the proposed application, acquiring accurately labeled training data (whether and which head movement took place) and monitoring of the quality of the predictive model can happen on the fly. From the perspective of HMD technology and Intelligent User Interfaces, the proposed BCI potentially improves user experience and enables new types of immersive applications.</abstract><author_list>Anne-Marie Brouwer, Jasper S. van der, Maarten A. Hogervorst, Alessia Cacace, Hans Stokking</author_list></RECORD><RECORD><contentID>12614</contentID><title>A New Experimental Paradigm for Affective Research in Neuro-adaptive Technologies</title><contentType>Workshop Paper</contentType><presentationID>10729</presentationID><abstract>One core challenge in the field of neuro-adaptive technology is the detection of the current mental user state. Existing experimental paradigms use established stimulus material (e.g. pictures) to induce affective user states and make them measurable. Since these paradigms lack ecological validity, there is a pressing need to design more interactive stimulus material that allows a reliably and systematical induction of different affective user states in more realistic scenarios. We present and empirically validate a new experimental paradigm featuring a simulated adaptive system that induces positive and negative affective user states through supporting or impeding goal achievement during a navigation task. Furthermore, we tested the feasibility of quantifying underlying neurophysiological processes of affective states by simultaneous investigations of electroencephalographic and functional near-infrared spectroscopic. These investigations further show the effectiveness of our paradigm in inducing different levels of affect and provides an indication of features of brain activity containing discriminative information, a proposal that warrants further investigation in a larger cohort of participants.</abstract><author_list>Kathrin Pollmann, Daniel Ziegler, Matthias Peissner, Mathias Vukelic</author_list></RECORD><RECORD><contentID>12672</contentID><title>A Questionnaire-based Case Study on Feedback by a Tangible Interface</title><contentType>Workshop Paper</contentType><presentationID>10758</presentationID><abstract>This paper presents an intelligent tangible user interface used for learning in a collaborative setting in a secondary school environment. We describe our explorative case study with 48 participants and report the results of a questionnaire filled in after the end of the case study. The questionnaire is related to feedback modalities, such as visual, audio/aural, and haptic, which the participants would like to have offered in the future by the tangible interface or the tangible objects.</abstract><author_list>Dimitra Anastasiou, Eric Ras</author_list></RECORD><RECORD><contentID>12625</contentID><title>A Survey of Definitions and Models of Exploratory Search</title><contentType>Workshop Paper</contentType><presentationID>10572</presentationID><abstract>Exploratory search has an unclear and open-ended definition. The complexity of the task and the difficulty of defining this activity are reflected in the limits of existing evaluation methods for exploratory search systems. In order to improve them, we intend to design an evaluation method based on a user-centered model of exploratory search. In this work, we identified and defined the characteristics of exploratory search and used them as an information seeking model evaluation grid. We tested this analytic grid on two information seeking models: Ellis&#xE2;?? and Marchionini&#xE2;??s models. The results show that Marchonini&#xE2;??s model does not match our evaluation method&#xE2;??s requirements whereas on the other hand Ellis&#xE2;?? model could be adapted to better suit exploratory search.</abstract><author_list>Emilie Palagi, Fabien Gandon, Alain Giboin, Rapha&#xEB;l Troncy</author_list></RECORD><RECORD><contentID>12629</contentID><title>Active Learning with Visualization for Text Data</title><contentType>Workshop Paper</contentType><presentationID>10722</presentationID><abstract>Labeled datasets are always limited, and oftentimes the quantity of labeled data is a bottleneck for data analytics. This especially affects supervised machine learning methods, which require labels for models to learn from the labeled data. Active learning algorithms have been proposed to help achieve good analytic models with limited labeling efforts, by determining which additional instance labels will be most beneficial for learning for a given model. Active learning is consistent with interactive analytics as it proceeds in a cycle in which the unlabeled data is automatically explored. However, in active learning users have no control of the instances to be labeled, and for text data, the annotation interface is usually document only. Both of these constraints seem to affect the performance of an active learning model. We hypothesize that visualization techniques, particularly interactive ones, will help to address these constraints. In this paper, we implement a pilot study of visualization in active learning for text classification, with an interactive labeling interface. We compare the results of three experiments. Early results indicate that visualization improves high-performance machine learning model building with an active learning algorithm.</abstract><author_list>Lulu Huang, Stan Matwin, Eder J. de Carvalho, Rosane Minghim</author_list></RECORD><RECORD><contentID>12727</contentID><title>Adaptive visual exploration for scientific paper writing</title><contentType>Workshop Paper</contentType><presentationID>10718</presentationID><abstract>Adaptive visual exploration for scientific paper writing</abstract><author_list>Cecilia di Sciascio, Lukas Mayr, Eduardo E. Veas</author_list></RECORD><RECORD><contentID>12685</contentID><title>AFRL Background and Goals</title><contentType>Workshop Paper</contentType><presentationID>10728</presentationID><abstract>AFRL Background and Goals
</abstract><author_list>Christopher McClernon</author_list></RECORD><RECORD><contentID>12677</contentID><title>An Evaluation of Hybrid Stacking on Interactive Tabletops</title><contentType>Workshop Paper</contentType><presentationID>10746</presentationID><abstract>Stacking is a common practice of organizing documents in the physical world. With the recent advent of interactive tabletops, physical documents can now coexist with digital documents on the same surface. As a result, systems were developed and studied which allow piling of both types of documents with the physical documents being placed on top of the digital ones. In this paper, we study the concept of true hybrid stacking, allowing users to stack both types of documents in an arbitrary order using a hybrid tabletop system called StackTop. We discuss the results and derive implications for future hybrid tabletop systems with stacking support.</abstract><author_list>Jan Riemann, Florian M&#xFC;ller, Sebastian G&#xFC;nther, Max M&#xFC;hlh&#xE4;user</author_list></RECORD><RECORD><contentID>12684</contentID><title>Applications of BCI Technology Beyond Communication And Control</title><contentType>Workshop Paper</contentType><presentationID>10727</presentationID><abstract>Applications of BCI Technology Beyond Communication And Control</abstract><author_list>Benjamin Blankertz</author_list></RECORD><RECORD><contentID>12732</contentID><title>Assessing Food Experience Through Physiological Measures</title><contentType>Workshop Paper</contentType><presentationID>10733</presentationID><abstract>Assessing Food Experience Through Physiological Measures</abstract><author_list>Maarten Hogervorst</author_list></RECORD><RECORD><contentID>12617</contentID><title>BCI for Physiological Text Annotation</title><contentType>Workshop Paper</contentType><presentationID>10730</presentationID><abstract>Automatic annotation of media content has become a critically important task for many digital services as the quantity of available online media content has grown exponentially. One approach is to annotate the content using the physiological responses of the media consumer. In the present paper, we reflect on three case studies that use brain signals for implicit text annotation to discuss the challenges faced when bringing passive brain-computer interfaces for physiological text annotation to the real world.</abstract><author_list>Oswald Barral, Ilkka Kosunen, Tuukka Ruotsalo</author_list></RECORD><RECORD><contentID>12640</contentID><title>Building Physical Props for Imagining Future Recommender Systems</title><contentType>Workshop Paper</contentType><presentationID>10767</presentationID><abstract>We propose the use of physical objects and hypothetical mock-ups in the conceptual development of algorithms and system functionality. This extends approaches like participatory design beyond the design process of interfaces, but rather allows for imagination of future algorithm functionality and reveals desiderata of systems outside the boundaries of existing systems.We demonstrate this approach using the outcomes of a series of interviews with users in the creative domain, more specifically, music production experts.We show three exemplary props built from cardboard embodying ideas emerged from interview sessions and how these, in turn, inspire conversations on future recommender systems, sound search engines, and sound manipulation interfaces.</abstract><author_list>Peter Knees, Kristina Andersen</author_list></RECORD><RECORD><contentID>12682</contentID><title>BYO*: Utilizing 3D Printed Tangible Tools for Interaction on Interactive Surfaces</title><contentType>Workshop Paper</contentType><presentationID>10745</presentationID><abstract>Sharing and manipulating information are essential for collaborative work in meeting scenarios. Nowadays, people tend to bring their own devices as a result of increasing mobility possibilities. However, transferring data from one device to another can be cumbersome and tedious if restrictions like different platforms, form factors or environmental limitations apply.  In this paper, we present two concepts to enrich interaction on and between devices through 3D printed customized tangibles: 1) Bring your own information, and 2) bring your own tools. For this, we enable interactivity for low-cost and passive tangible 3D printed objects by adding conductive material and make use of touch-enabled surfaces. Our system allows users to easily share digital contents across various devices and to manipulate them with individually designed tools without additional hardware required.</abstract><author_list>Sebastian G&#xFC;nther, Martin Schmitz, Florian M&#xFC;ller, Jan Riemann, Max M&#xFC;hlh&#xE4;user</author_list></RECORD><RECORD><contentID>12674</contentID><title>Conceptual Motivation Modeling for Students with Dyslexia for Enhanced Assistive Learning</title><contentType>Workshop Paper</contentType><presentationID>10753</presentationID><abstract>Students with dyslexia often suffer from the lack of academic self-worth and frustration that can even lead to learned helplessness. However, few studies have investigated the impact of incorporating users&#xE2;?? motivational factors into user modeling to enhance the learning experience for students with dyslexia. In this paper we attempt to develop a conceptual motivation model for people with dyslexia in their use of assistive learning tools. To this end, we carry out a small-scale empirical study in real-world learning scenarios. Then we conduct individual interviews to gain first-hand data about the key factors and features affecting learning experience.  Using coding and thematic analysis methods we discuss main themes regarding motivational factors and their interrelationships identified. Based on these findings, we create a conceptual motivation model tailored towards students with dyslexia, which will help enhance learning experience and improve learning efficiency.</abstract><author_list>Ruijie Wang, Liming Chen, Ivar Solheim, Trenton Schulz, Aladdin Ayesh</author_list></RECORD><RECORD><contentID>12671</contentID><title>Designing a System for Playful Coached Learning in the STEM Curriculum</title><contentType>Workshop Paper</contentType><presentationID>10757</presentationID><abstract>We present the design outline of a context-aware interactive system for smart learning in the STEM curriculum (science, technology, engineering, and mathematics). It is based on a gameful design approach and enables &#xE2;??playful coached learning&#xE2;?&#x9D; (PCL): a learning process enriched by gamification but also close to the learner&#xE2;??s activities and emotional setting. After a brief introduction on related work, we describe the technological setup, the integration of projected visual feedback and the use of object and motion recognition to interpret the learner&#xE2;??s actions. We explain how this combination enables rapid feedback and why this is particularly important for correct habit formation in practical skills training. In a second step, we discuss gamification methods and analyze which are best suited for the PCL system. Finally, emotion recognition, a major element of the final PCL design not yet implemented, is briefly outlined.</abstract><author_list>Oliver Korn, Adrian Rees, Alan Dix</author_list></RECORD><RECORD><contentID>12700</contentID><title>Enabling Awareness in Playful Environments for Animals using Body Tracking</title><contentType>Workshop Paper</contentType><presentationID>10705</presentationID><abstract>This paper explores the diverse possibilities for awareness that could be implemented in the development of intelligent playful environments for animals. These opportunities are described from the perspective of both the animal and the human playing, based on the interaction with the system through embodied interactions using non-wearable tracking.</abstract><author_list>Patricia Pons, Javier Jaen</author_list></RECORD><RECORD><contentID>12710</contentID><title>Enhancing Social Inclusion between Drivers by Digital Augmentation</title><contentType>Workshop Paper</contentType><presentationID>10704</presentationID><abstract>The physical structure of vehicles induces a tendency to depersonalize other drivers, which may lead to aggressive driving and social isolation. With everywhere available connectivity and the broad penetration of social network services, the relationship between drivers on the road may gain more transparency, enabling social information to pass through the steel shell of the cars and giving opportunities to reduce anonymity and strengthen empathy. In this study, we introduced two social concepts on the road, which utilize the latest Vehicle to Vehicle communication technology. Furthermore, two corresponding prototypes  on a driving simulator were developed for furtherexploration to get the insights of social awareness between drivers by digital augmentation.</abstract><author_list>Chao Wang, Jacques Terken, Jun Hu</author_list></RECORD><RECORD><contentID>12632</contentID><title>EulerianGrapher: Text Visualisation at the Level of Character N--grams based on Eulerian Graphs</title><contentType>Workshop Paper</contentType><presentationID>10723</presentationID><abstract>Network analysis has been applied widely in different domains, providing a unified language to describe disparate domains. In this paper we present an interactive visualisation of graph--based ranking measures of text character n--grams. The visualisation provides insights into overall characteristics of the observed text, and an interactive manipulation of different parameters used to adjust, filter or emphasize relevant n--grams in the graph.</abstract><author_list>Dijana Kosmajac, Vlado Keselj, Evangelos Milios</author_list></RECORD><RECORD><contentID>12624</contentID><title>Exploring Scientific Literature Search through Topic Models</title><contentType>Workshop Paper</contentType><presentationID>10721</presentationID><abstract>With the fast growing amount of scientific literature, browsing through it can be a dicult task: formulating a precise query may be problematic as new research areas emerge quickly and different terms are often used to describe the same concept. To tackle some of these issues, we built a system for exploratory scientific search based on topic models. An initial short user study shows that through visualizing the relationship between keyphrases, documents and authors, the system allows the user to better explore the document search space compared to traditional systems based solely on search query.</abstract><author_list>Amin Sorkhei, Kalle Ilves, Dorota Glowacka</author_list></RECORD><RECORD><contentID>12637</contentID><title>Game-Based Extraction of Web Users\' Personality Factors for Personalization</title><contentType>Workshop Paper</contentType><presentationID>10765</presentationID><abstract>The volume of information users are exposed to on the web is overwhelming. To increase effectiveness of information delivery to users, providers employ personalization strategies. In a highly competitive environment, simplistic strategies do not suffice, and high-quality   personalization is required. These can be based on users&#xE2;?? decision making models. To build such models, we need to extract factors of direct influence on users&#xE2;?? decision making. Personality factors are known to have this direct influence.  They are stable over time and across situations, and they assist in predicting future behavior of individuals in a scientific way. In this paper, we introduce a novel methodology for extracting users\' personality factors without holding any prior information on the users\' behavior and, notably, without administering any psychological questionnaires. This allows us to build a designated model for each user or users&#xE2;?? group, and in turn facilitates effective personalized information delivery.</abstract><author_list>Rachel Yahel Halfon, Onn Shehory, David G. Schwartz</author_list></RECORD><RECORD><contentID>12711</contentID><title>Getting Aware of the Potential Success of the Classifier through Visualizations</title><contentType>Workshop Paper</contentType><presentationID>10685</presentationID><abstract>Building a successful machine learning classifier requires a large amount of work. It is important to be aware of the classifier&#xE2;??s probability of success at an early stage to make sure that the cost is well-spent. In this position paper, we describe our experience in building Label-and-Learn, a visualization interface that helps users establish awareness of the characteristics of the machine learning problem as  early as the data labeling process, so that they could effectively adjust their strategies. We would also discuss the success and failures of our design based on user study results.</abstract><author_list>Yunjia Sun, Edward Lank, Michael Terry</author_list></RECORD><RECORD><contentID>12633</contentID><title>HiveRel: Towards Focused Knowledge Acquisition</title><contentType>Workshop Paper</contentType><presentationID>10574</presentationID><abstract>In this paper we argue that focused knowledge acquisition -- knowledge acquisition in user-defined query context -- can be best achieved throughpresentation of relationships between search results for a given user query. Wesuggest that in order to expose relationships while maintaining relevance-basedlayout of search results, presentation of search results could benefit from using a two-dimensional layout instead of the conventional one-dimensional rankedlist . We introduce HiveRel, a search system that presents search results as tiled hexagons on a map-like surface with center-out relevance ordering, in which relationships are displayed on-demand. A user study compared HiveRel to standard web search over a range of focused knowledge acquisition tasks and two different domains. The results indicate that users completed the tasks faster and with comparable quality using HiveRel, and rated it positively.</abstract><author_list>Sivan Yogev, Guy Shani, Noam Tractinsky</author_list></RECORD><RECORD><contentID>12680</contentID><title>Hot Under the Collar? Building a Perceptive Home Energy Interface for Chemotherapy Recipients</title><contentType>Workshop Paper</contentType><presentationID>10748</presentationID><abstract>Smart home technology development has seen a rise in popularity in recent years, with increased availability of applications that can control lighting, security systems, and residence temperature from anywhere. Despite this, home energy management systems (HEMS) do not currently consider the dynamic nature of household structure and their residents. This research discusses design implications for building a home energy management system that can interact with situational variants and assist in temperature decision making, relieving users of added stressors during times of familial changes. Aspects of this perceptive home energy management system considers household structures and changing needs when faced with a new heath situation, specifically a cancer diagnosis or menopausal side effects. These systems should be built to assist in decision making including how to set home temperature in order to maximize personal comfort and alleviate stress.</abstract><author_list>Germaine Irwin</author_list></RECORD><RECORD><contentID>12681</contentID><title>In-vehicle Human Machine Interface: An Approach to Enhance Eco-Driving Behaviors</title><contentType>Workshop Paper</contentType><presentationID>10744</presentationID><abstract>In the context of behavioral change for a more sustainability mobility, we designed and implemented an in-vehicle human machine interface for electric vehicles, on the basis of an approach we propose that exploits gamification and machine learning techniques. Our main goal is to equip the driver with instant and accurate eco-driving strategies, obtaining an optimization of the energy consumption. More specifically, we have developed a prototype that collects data related to the driver\'s braking style and makes use of a machine learning model to forward-predict the resulting energy gain. It then accordingly fosters custom eco-driving behaviour by means of gamified interactions provided on an infotainment dashboard on the car. We have conducted some tests and this paper presents the preliminary and promising results we obtained.</abstract><author_list>Pietro Di Lena, Silvia Mirri, Catia Prandi, Paola Salomoni, Giovanni Delnevo</author_list></RECORD><RECORD><contentID>12675</contentID><title>Integrating Social Media into an Instructional Design Support System</title><contentType>Workshop Paper</contentType><presentationID>10754</presentationID><abstract>This paper is the result of collaboration between the University of Genoa and Middle East Technical University Northern Cyprus Campus about the enhancement of a recommender system that supports academics to integrate social networking and Web 2.0 tools for language education. Starting from a survey about current usage of social media in education and language teaching, the paper describes the extension of the L-Max Ontology to support teachers in building learning activities using Twitter, in relation to specific competences to be developed, specific learning goals and contextual constraints. To test the extended version of the recommender we performed a mockup evaluation based on the L-Max Ontology.</abstract><author_list>Ilknur Celik, Ilaria Torre, Simone Torsani</author_list></RECORD><RECORD><contentID>12636</contentID><title>Intelligent User Interfaces for Social Music Discovery and Exploration of Large-scale Music Repositories</title><contentType>Workshop Paper</contentType><presentationID>10763</presentationID><abstract>In this position paper, we address the question of how to make music search and discovery more appealing, more exciting, and more joyful.In particular, we argue to research methods that foster serendipitous encounters with music items and to integrate ways for social interaction while exploring music collections and discovering the gems in today\'s huge catalogs available through online streaming platforms.We identify two major challenges here: the need for (i) highly efficient clustering and information visualization techniques that scale to these music catalogs and (ii) novel user interfaces that explain the clustering of music items and provide means to make the exploration of music a social event.</abstract><author_list>Markus Schedl</author_list></RECORD><RECORD><contentID>12678</contentID><title>Intelligent Visual Interface with the Internet of Things</title><contentType>Workshop Paper</contentType><presentationID>10747</presentationID><abstract>Communication between users and physical objects and sensors through the web within the Internet of Things framework, requires by definition the capability to perceive the sensors and the underlying information and services. Visualization of the Things in IoT is thus a requirement for natural interaction between users and IoT instances in the upcoming but steadily established computing paradigm. The immense quantity of sensors and variety of usable information introduces the need to intelligently filter and adapt the respective information sources and layers. Current work proposes an architecture that supports intelligent interaction between users and the IoT addressing the intelligent perception requirement described earlier. On the one hand, sensory visualization is tackled via Augmented Reality layers of sensors and information and on the other hand context and location awareness enhance the system by providing usable in the respective senses information.</abstract><author_list>Konstantinos Michalakis, John Aliprantis, George Caridakis</author_list></RECORD><RECORD><contentID>12696</contentID><title>Interface Design for Interactive Sound Event Detection</title><contentType>Workshop Paper</contentType><presentationID>10680</presentationID><abstract>To label sound events (e.g. "dog bark") in an hours-long audio file quickly and accurately, an interactive human-inthe-loop paradigm can be applied to sound event detection. While interactive learning has been used in many other labeling tasks (e.g. image or text), sound event detection requires a different interface approach. In this article, we discuss principles of effective interaction design for interactive sound event detection and labeling, using examples from our work in this area.</abstract><author_list>Bongjun Kim, Bryan Pardo</author_list></RECORD><RECORD><contentID>12641</contentID><title>Modeling Characteristics of Location from User Photos</title><contentType>Workshop Paper</contentType><presentationID>10762</presentationID><abstract>In the past decade, location-based services have grown through geo-tagging and place-tagging. Proliferation of GPS-enabled mobile devices further enabled exponential growth in geo-tagged user content. On the other hand, location-based applications harness the abundance of geo-tagged content to further improve user experience and more relevant localized content. We show in this paper that geo-tagged content can vary significantly based on whether they are captured by a local versus a tourist to the location. Using photos shared by online users, we also show how we can learn unique characteristics about a given location. We finally discuss an effective metric to rank the most representative photos for a given location by combining visual contents and their social engagement potential.</abstract><author_list>Vikas Kumar, Saeideh Bakhshi, Lyndon Kennedy, David A. Shamma</author_list></RECORD><RECORD><contentID>12628</contentID><title>Music Exploration by Impression Based Interaction</title><contentType>Workshop Paper</contentType><presentationID>10720</presentationID><abstract>Search and recommendation systems help users find their favorites among an abundant amount of songs available on distributors such as iTunes.Nevertheless, it is still hard for us to efficiently retrieve the right music. This paper proposes an interactive music exploration system to help them reach their favorite songs by means of interactive feedbacks submitted by the user. The recommendation component of our system is based on the feedback component accepts the user\'s feedbacks in terms of impression features such as \</abstract><author_list>Hiroki Fujino, Koiti Hasida, Yusuke Matsubara</author_list></RECORD><RECORD><contentID>12615</contentID><title>Neuroadaptive Meditation in the Real World</title><contentType>Workshop Paper</contentType><presentationID>10734</presentationID><abstract>Meditation and mindfulness techniques are useful for both treatment of various disorders as well as improving the quality of life in general. Meditation offers intriguing possibilities for BCI as it is targeted at able-bodied general population and goes beyond the traditional explicit control BCI paradigm.  In previous work, we have shown how neurofeedback can be successfully applied in a laboratory setting to improve the meditation experience. This position paper aims to expand this work in two ways. First, we explore the problems and issues that might arise when moving from the laboratory setting to the normal, everyday world. Second, we will consider the possibilities of extending the neurofeedback with other forms of physiological computing. Our position is that meditation and relaxation applications provide a perfect application area for bringing BCI into the real world.</abstract><author_list>Ilkka Kosunen, Antti Ruonala, Mikko Salminen, Simo J&#xE4;rvel&#xE4;, Niklas Ravaja, Giulio Jacucci</author_list></RECORD><RECORD><contentID>12693</contentID><title>On User Awareness in Model-based Collaborative Filtering Systems</title><contentType>Workshop Paper</contentType><presentationID>10710</presentationID><abstract>In this paper, we discuss several aspects that users are typically not fully aware of when using model-based Collaborative Filtering systems. For instance, the methods prevalently used in conventional recommenders in-fer abstract models that are opaque to users, making it difficult to understand the learned profile, and conse-quently, why certain items are recommended. Further, users are not able to keep an overview of the item space, and thus the alternatives that in principle could also be suggested. By summarizing our experiences on exploiting latent factor models for increasing control and transparency, we show that the respective tech-niques may also contribute to make users more aware of their preferences&#xE2;?? representation, the rationale be-hind the results, and further items of potential interest.</abstract><author_list>Benedikt Loepp, J&#xFC;rgen Ziegler</author_list></RECORD><RECORD><contentID>12639</contentID><title>Personalized Gaming for Motivating Social and Behavioral Science Participation</title><contentType>Workshop Paper</contentType><presentationID>10766</presentationID><abstract>Game-like environments are increasingly used for conducting research due to the affordances that such environments offer. However, the problem remains that such environments treat their users equally. In order to address this, personalization is necessary. In this paper we discuss the need to personalize gamified research environments to motivate participation by illustrating a playful platform called Mad Science, which is being developed to allow users to create social and behavioral studies. This discussion is both informed by the platform\'s affordances and use thus far as well as existing theories on player motivation, and contributes to theory-informed approaches to (gamified) personalization technologies.</abstract><author_list>Casper Harteveld, Steven C. Sutherland</author_list></RECORD><RECORD><contentID>12638</contentID><title>Personalizing Online Educational Tools</title><contentType>Workshop Paper</contentType><presentationID>10764</presentationID><abstract>As more people turn to online resources to learn, there will be an increasing need for systems to understand and adapt to the needs of their users. Engagement is an important aspect to keep users committed to learning. Learning approaches for online systems can benefit from personalization to engage their users. However, many approaches for personalization currently rely on methods (e.g., historical behavioral data, questionnaires, quizzes) that are unable to provide a personalized experience from the start-of-use of a system. As users in a learning environment are exposed to new content, the first impression that they receive from the system influences their commitment with the program. In this position paper we propose a quantitative approach for personalization in online learning environments to overcome current problems for personalization in such environments.</abstract><author_list>Michael J Lee, Bruce Ferwerda</author_list></RECORD><RECORD><contentID>12613</contentID><title>Physiological Effects of Adaptive Cruise Control Behaviour in Real Driving</title><contentType>Workshop Paper</contentType><presentationID>10731</presentationID><abstract>We examined physiological responses to behavior of an Adaptive Cruise Control (ACC) system during real driving. ACC is an example of automating a task that used to be performed by the user. In order to preserve the link between the user and an automated system such that they work together optimally, physiological signals reflecting mental state may be useful. We asked 15 participants to use an ACC at designated times while driving a track. When the ACC was activated, the car decelerated either strongly or softly, which was either according to expectation or not. Heart rate, eye blinks, and brain signals (EEG) were recorded. Heart rate and blink duration were the same following the announcement of an upcoming expected or unexpected deceleration profile. Heart rate and blink duration increased when a strong compared to a soft deceleration profile was announced, consistent with a state of arousal or startle. This was only found for the first half of the trials, when the driver was expected to be more alert and engaged (as also evidenced by decreasing heart rate, and increasing EEG alpha and blink duration over the trials). We conclude that for ACC behavior that is relevant for the driver, heart  rate and blink duration may be used as a source of information about mental state elicited by the ACC, which could be used to evaluate driving experience.</abstract><author_list>Anne-Marie Brouwer, Anne Snelting, Matthew Jaswa, Oded Flascher, Laurens Krol, Thorsten Zander</author_list></RECORD><RECORD><contentID>12616</contentID><title>Real World BCI: Cross-Domain Learning and Practical Applications</title><contentType>Workshop Paper</contentType><presentationID>10737</presentationID><abstract>In order to develop real-world BCI solutions machine learning models must generalize not only to unseen users but also to unseen scenarios. In this concept paper we describe our initial investigation into Deep Learning tools to create generalized models for both cross-subject and cross-domain learning. We demonstrate our approach using two different, laboratory grade data sets to train a learning model that we then apply to a third more complex scenario. While our results indicate that cross-domain learning is possible, we also identify potential avenues for further research and development (such as disentangling spatially or temporally overlapping responses). Finally, we describe our work to implement a system that uses cross-domain learning to develop a real-time application for performing BCI-based Human-Centric Scene Analysis.</abstract><author_list>Stephen M. Gordon, Matthew Jaswa, Amelia J. Solon, Vernon J. Lawhern</author_list></RECORD><RECORD><contentID>12669</contentID><title>Sensor-based Methodological Observations for Studying Online Learning</title><contentType>Workshop Paper</contentType><presentationID>10756</presentationID><abstract>Online learning has gained increased popularity in recent years. However, with online learning, teacher observation and intervention is lost, creating a need for technologically observable characteristics that can compensate for this limitation. The present study used a wide array of sensing mechanisms including eye tracking, galvanic skin response (GSR) recording, facial expression analysis, and summary note-taking to monitor participants while they watched and recalled an online video lecture. We explored the link between these human-elicited responses and learning outcomes as measured by quiz questions. Results revealed GSR to be the best indicator of the challenge level of the lecture material. Yet, eye tracking and GSR remain difficult to capture when monitoring online learning as the requirement to remain still impacts natural behavior and leads to more stoic and unexpressive faces. Continued work on methods ensuring naturalistic capture are critical for broadening the use of sensor technology in online learning, as are ways to fuse these data with other input, such as structured and unstructured data from peer-to-peer or student-teacher interactions.</abstract><author_list>Ashley A Edwards, Anthony Massicci, Srinivas Sridharan, Joe Geigel, Linwei Wang, Reynold Bailey</author_list></RECORD><RECORD><contentID>12694</contentID><title>Should Context-aware IDE Command Recommendations Always Be Presented In-context or Not</title><contentType>Workshop Paper</contentType><presentationID>10711</presentationID><abstract>Aiming at improving software developers&#xE2;?? knowledge of their selected IDE, recommender system technologies have been recently introduced. In this short paper, we  argue that IDE command recommendations must be context-aware in order to be accepted by the users, but they do not have to be necessarily presented in the context in which they can be executed. Instead, we suggest that the system allows the users to browse recommendations when they find it more convenient.</abstract><author_list>Marko Gasparic, Francesco Ricci</author_list></RECORD><RECORD><contentID>12670</contentID><title>Supporting Knowledge Sharing and Learning via Semantic Geographical Maps</title><contentType>Workshop Paper</contentType><presentationID>10751</presentationID><abstract>Map-based applications are a good starting point for helping teachers in the preparation of learning material and students in their researches in social sciences. However, they offer basic information filtering support to the generation of dynamic maps. In this paper, we investigate the adoption of semantic knowledge representation and cooperative work approaches for managing thematic maps in group-based learning activities. Moreover, we present a possible solution, based on the OnToMap Participatory GIS, which uses an ontological representation of geographical information to support multi-faceted information retrieval, crowdsourcing, and map creation.</abstract><author_list>Liliana Ardissono, Noemi Mauro, Adriano Savoca</author_list></RECORD><RECORD><contentID>12619</contentID><title>The BrainHack Project: Exploring Art - BCI Hackathons</title><contentType>Workshop Paper</contentType><presentationID>10732</presentationID><abstract>The main goal of the BrainHack project is to engage the international artistic community experimenting with Brain Neural Computer Interaction (BNCI) technologies and link it to the BNCI scientific community. BrainHack explores hackathons format to enhance the experimentation with non-clinical and artistic uses of BNCI. Here we briefly summarize the two \</abstract><author_list>Aleksander Ohrn, Lucas Evers, Brendan Z. Allison, Jurre Ongering, Angela Riccio, Irene Igardi</author_list></RECORD><RECORD><contentID>12618</contentID><title>The Expectation Based Eye-Brain-Computer Interface: An Attempt of Online Test</title><contentType>Workshop Paper</contentType><presentationID>10736</presentationID><abstract>In this preliminary study we tested online a new Eye-Brain-Computer Interface (EBCI) for selection of positions on a screen with a combination of gaze based control and a passive brain-computer interface (BCI). This hybrid BCI was trained offline to recognize the electroencephalogram (EEG) patterns recorded during gaze dwells intentionally used to make moves in a computer game. The patterns were presumably related to expectation of the interface feedback. In the online test, 500 ms gaze dwells led to actions each time the BCI classified them as intentional. When the BCI made a miss, a participant could still communicate the intention by prolonging the dwell up to 1000 ms. Also playing the game was possible, it was found that defining the ground truth for such an online system is not trivial and that further efforts will be needed to evaluate the performance of the expectation based EBCI reliably.</abstract><author_list>Yuri O. Nuzhdin, Sergei L. Shishkin, Anastasia A. Fedorova, Alexander G. Trofimov, Evgeny P. Svirin, Bogdan L. Kozyrskiy</author_list></RECORD><RECORD><contentID>12698</contentID><title>The Presentation of Synthesized Artifacts in Example-driven Interfaces</title><contentType>Workshop Paper</contentType><presentationID>10684</presentationID><abstract>Example-driven interfaces (EDIs) for data tasks represent a class of mixed-initiative user interfaces that enable users to specify tasks such as data extraction, cleaning, trans- formation, querying or analysis, in the casual and familiar language of examples. From user-provided examples of intended task behavior, EDIs synthesize programs, scripts or rules, that can perform the task in a fashion consistent with the examples. In this position paper, we examine the design of the interface component that presents the synthesized artifacts to the users.</abstract><author_list>Azza Abouzied</author_list></RECORD><RECORD><contentID>12703</contentID><title>The Presentation of Synthesized Artifacts in Example-driven Interfaces</title><contentType>Workshop Paper</contentType><presentationID>10687</presentationID><abstract>Example-driven interfaces (EDIs) for data tasks represent a class of mixed-initiative user interfaces that enable users to specify tasks such as data extraction, cleaning, trans- formation, querying or analysis, in the casual and familiar language of examples. From user-provided examples of intended task behavior, EDIs synthesize programs, scripts or rules, that can perform the task in a fashion consistent with the examples. In this position paper, we examine the design of the interface component that presents the synthesized artifacts to the users.</abstract><author_list>Azza Abouzied</author_list></RECORD><RECORD><contentID>12679</contentID><title>The W3C MMI Architecture in the Context of the Smart Car</title><contentType>Workshop Paper</contentType><presentationID>10743</presentationID><abstract>With the GENIVI project, an open source approach to ease the development of scalable in-vehicle infotainment systems is available. However, in its current state it only provides limited capabilities when it comes to the addition of new modalities in the smart car. A possible solution is available in the form of the W3C MMI architectural pattern. In this paper, we analyze a potential combination of these two efforts.</abstract><author_list>Dirk Schnelle-Walka, Stefan Radomski</author_list></RECORD><RECORD><contentID>12697</contentID><title>The What, When and How of Awareness for Effective Learning in Surgical Simulation</title><contentType>Workshop Paper</contentType><presentationID>10706</presentationID><abstract>Fine motor skill is indispensable for dental surgeons as every movement needs to be precise given the minute margins of error in endodontic surgery. Training in manual dexterity and proper instrument handling are crucial components in the dental curriculum. During training, the students need pay close attention on the undertaken actions and need to be aware of incorrect or inappropriate manipulation of instruments and their consequences. To create awareness of errors, we propose a feedback mechanism in a VR dental surgical simulator. To achieve effective learning, we focus on three aspects of feedback as What (Feedback content on error), When (Feedback timing) and How (Feedback modality).</abstract><author_list>Myat Su Yin, Peter Haddawy</author_list></RECORD><RECORD><contentID>12695</contentID><title>Towards Cognitive Awareness: A Mobile Context Modeling- and Notification-based Approach</title><contentType>Workshop Paper</contentType><presentationID>10683</presentationID><abstract>Awareness about one&#x2019;s own cognitive state is a critical first step towards better physical, physiological, psychological, behavioral, and social health. When users are aware of their mental states &#x2013; particularly those which might be detrimental to their health (e.g. high stress, low excitement) &#x2013; they can take the necessary steps to either alter their behavior, or find ways to effectively cope with it. From a technology viewpoint, it is thus important to find ways to detect a user&#x2019;s cognitive state as unobtrusively as possible, and thereafter either inform or assist the users in coping with it. Through this position paper, we wish to discuss the potential of using mobile context data gathered from a user&#x2019;s smartphone to infer the user&#x2019;s cognitive state, and thereafter using mobile notifications to deliver timely intervention messages.</abstract><author_list>Akhil Mathur, Fahim Kawsar</author_list></RECORD><RECORD><contentID>12699</contentID><title>User Awareness in Expressing Emotional Intensity using Touchscreen Gestures</title><contentType>Workshop Paper</contentType><presentationID>10682</presentationID><abstract>Understanding the varying intensities of human emotions are crucial to maintain a proper workflow in day to day activities. There are numerous factors that need to be considered to realize what makes people aware of their state of mind and how they feel. We conducted a participatory design study with recreational runners, followed by a controlled study to explore the awareness of how people use touchscreens to express emotional intensities. We discuss user awareness of emotional expression through gestures during common activities.</abstract><author_list>Nabil Bin Hannan, Derek Reilly</author_list></RECORD><RECORD><contentID>12630</contentID><title>VIQS: Visual Interactive Exploration of Query Semantics</title><contentType>Workshop Paper</contentType><presentationID>10575</presentationID><abstract>Analytics platforms such as IBM\'s Watson Analytics TM are collecting metadata about their use, including user queries on uploaded datasets.  The analysis of this metadata may be valuable in improving services, such as query recommendation and automatic data visualization.However, analysis of metadata is difficult not only in terms of scale but also in terms of complexity.Generalizing and exploring query patterns across users and datasets is challenging. Abstractions are likely to help bridge differences in specifics (e.g., column namesand query details), particularly in semantics. For example, a single query, \</abstract><author_list>Christina Christodoulakis, Eser Kandogan, Ignacio G Terrizzano, Ren&#xE9;e J Miller</author_list></RECORD><RECORD><contentID>12673</contentID><title>Virtual Reality for Special Educational Needs</title><contentType>Workshop Paper</contentType><presentationID>10752</presentationID><abstract>Virtual Reality (VR) can be viewed as an assistive technology, due to its potential to minimize or offset the effects of a disability and provide an alternative mean for an individual to accomplish a particular task [12]. It is a promising avenue to provide children with Special Educational Needs (SENs) opportunities that they otherwise would never experience. VR learning environments can be personalized to allow a child to focus on their unique strengths and abilities, rather than limit their interactive capabilities, and work toward mastery of a task. VR can provide a safe and supportive simulated environment that allows a child to practice or enhance various skills which can be transferred to the real world.VR encourage interactive learning and provide a variety of opportunities for the learner to have control over the learning process. In fact the flexibility and controllability of VR provides a vehicle for interactive, ecological and valid assessment tools in formal, informal or continuing education. Some advantages of these technologies are the repetition, the control over the learning process and, in addition, VR systems have the capability to increase the complexity of the tasks and measure performance [8].</abstract><author_list>Alberto Messina, Mario Chiesa, Riccardo Toppan</author_list></RECORD><RECORD><contentID>12627</contentID><title>Visual Exploration and Analysis of Recommender Histories</title><contentType>Workshop Paper</contentType><presentationID>10576</presentationID><abstract>Content based recommender systems are commonly applied to provide automatic support to users searching for relevant information. However, as the retrieved number of resources may grow large, and because the user does not have direct control over the search process, re-finding and analyzing the retrieved information can become a difficult task.We introduce the textit{ECHO} (textbf{E}xplorer of textbf{C}ollection textbf{H}isttextbf{O}ries) tool, which allows the user to visualize and analyze search result histories.Using an interactive three-dimensional scene, the history of recommender queries and the corresponding collections of recommended items can be easily explored.A multiple Levels of Detail approach empowers users to drill down starting from the overview of the query history, then performing visual meta data filtering on each result collection, down to detailed representation of single results. In particular, for each result collection we provide an interactive visualization of the meta data distribution. This representation makes it possible to apply global filters over the entire query history to identify additional valuable items in other result collections.Also, our approach supports the user in comparing the different collections and visually identifying similarities between them.textit{ECHO} is implemented as a web application. To avoid rendering performance issues and to guarantee smooth, animated transitions, we rely on graphic card acceleration using textit{WebGL} technology.</abstract><author_list>Peter Hasitschka, Vedran Sabol</author_list></RECORD><RECORD><contentID>12623</contentID><title>Visual Exploration of Large Scatter Plot Matrices by Pattern Recommendation based on Eye Tracking</title><contentType>Workshop Paper</contentType><presentationID>10573</presentationID><abstract>The Scatter Plot Matrix (SPLOM) is a well-known technique for visual analysis of high-dimensional data. However, one problem of large SPLOMs is that typically not all views are potentially relevant to a given analysis task or user. The matrix itself may contain structured patterns across the dimensions, which could interfere with the investigation for unexplored views. We introduce a new concept and prototype implementation for an interactive recommender system supporting the exploration of large SPLOMs based on indirectly obtained user feedback from user eye tracking. Our system records the patterns that are currently under exploration based on gaze times, recommending areas of the SPLOM containing potentially new, unseen patterns for successive exploration. We use an image-based dissimilarity measure to recommend patterns that are visually dissimilar to previously seen ones, to guide the exploration in large SPLOMs. The dynamic exploration process is visualized by an analysis provenance heatmap, which captures the duration on explored and recommended SPLOM areas. We demonstrate our exploration process by a user experiment, showing the indirectly controlled recommender system achieves higher pattern recall as compared to fully interactive navigation using mouse operations.</abstract><author_list>Lin Shao, Nelson Silva, Eva Eggeling, Tobias Schreck</author_list></RECORD><RECORD><contentID>12626</contentID><title>Visual Exploration of Network Hostile Behavior</title><contentType>Workshop Paper</contentType><presentationID>10724</presentationID><abstract>This paper presents a graphical interface to identify hostile behavior in network logs. The problem of identifying and labeling hostile behavior is well known in the network security community. There is a lack of labeled datasets, which make it difficult to deploy automated methods or to test the performance of manual ones. We describe the process of searching and identifying hostile behavior with a graphical tool derived from an open source Intrusion Prevention System, which graphically encodes features of network connections from a log-file. A design study with two network security experts illustrates the workflow of searching for patterns descriptive of unwanted behavior and labeling occurrences therewith.</abstract><author_list>Jorge Guerra, Carlos Adri&#xE1;n Catania, Eduardo Veas</author_list></RECORD><RECORD><contentID>12709</contentID><title>When Less is More: Semantic Awareness in Web Screen Reading</title><contentType>Workshop Paper</contentType><presentationID>10681</presentationID><abstract>Semantic awareness is the key to making the Web usable by blind people who use screen readers, an assistive technology, to read aloud digital content serially. A semantics-aware web screen reader (SRAA) elevates the interaction to a higher level of abstraction from operating on (syntactic) HTML elements, as is done now with a conventional screen reader, to operating on web entities (which are semantically meaningful collections of related HTML elements, e.g. search results, menus, widgets, etc.). With SRAA, users  can give commands such as &#xE2;??Read the article&#xE2;?&#x9D;, &#xE2;??Log in&#xE2;?&#x9D;, &#xE2;??Next result&#xE2;?&#x9D;, as well as interact with commonwidgets such as the Date-Picker by saying &#xE2;??Next month&#xE2;?&#x9D;, &#xE2;??Which day is March 13th&#xE2;?&#x9D;, etc. Doing so brings blind users closer to how sighted people perceive and operate over web entities.</abstract><author_list>Vikas Ashok, Yevgen Borodin, I.V. Ramakrishnan</author_list></RECORD></RECORDS>
